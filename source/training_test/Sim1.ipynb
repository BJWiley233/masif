{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting cache_dir to nn_models/sc05/cache/ \n",
      "Setting model_dir to nn_models/sc05/all_feat/model_data/ \n",
      "Setting desc_dir to output/sc05/all_feat/model_data/ \n",
      "Setting gif_eval_out to nn_models/sc05/gif_eval/ \n",
      "Setting min_sc_filt to 0.5 \n",
      "Setting max_sc_filt to 1.0 \n",
      "Setting pos_surf_accept_probability to 1.0 \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from IPython.core.debugger import set_trace\n",
    "import importlib\n",
    "import sys\n",
    "from default_config.masif_opts import masif_opts\n",
    "from masif_modules.MaSIF_ligand import MaSIF_ligand\n",
    "from masif_modules.MaSIF_ligand_Brian_tf1 import MaSIF_ligand2\n",
    "from masif_modules.read_ligand_tfrecords_tf1 import _parse_function\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf\n",
    "from default_config.masif_opts import masif_opts\n",
    "\n",
    "\n",
    "custom_params = {}\n",
    "custom_params['cache_dir'] = 'nn_models/sc05/cache/'\n",
    "custom_params['model_dir'] = 'nn_models/sc05/all_feat/model_data/'\n",
    "custom_params['desc_dir'] = 'output/sc05/all_feat/model_data/'\n",
    "custom_params['gif_eval_out'] = 'nn_models/sc05/gif_eval/'\n",
    "custom_params['min_sc_filt'] = 0.5\n",
    "custom_params['max_sc_filt'] = 1.0\n",
    "custom_params['pos_surf_accept_probability'] = 1.0\n",
    "\n",
    "params = masif_opts[\"site\"]\n",
    "params[\"n_conv_layers\"]=1\n",
    "for key in custom_params:\n",
    "    print(\"Setting {} to {} \".format(key, custom_params[key]))\n",
    "    params[key] = custom_params[key]\n",
    "\n",
    "\n",
    "params[\"max_distance\"]=5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6764"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "data = \"/data/pompei/bw973/Oxygenases/PHD2/PHD2_50_O2IF/Bundle/Sim1\"\n",
    "\n",
    "ppi_pair_ids=glob.glob(data+\"/data_preparation/04a-precomputation_12A/precomputation/Sim_1_traj_*\")\n",
    "ppi_pair_ids=[i.split('/')[-1] for i in ppi_pair_ids]\n",
    "len(ppi_pair_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ppi_pair_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "1000\n",
      "1050\n",
      "1100\n",
      "1150\n",
      "1200\n",
      "1250\n",
      "1300\n",
      "1350\n",
      "1400\n",
      "1450\n",
      "1500\n",
      "1550\n",
      "1600\n",
      "1650\n",
      "1700\n",
      "1750\n",
      "1800\n",
      "1850\n",
      "1900\n",
      "1950\n"
     ]
    }
   ],
   "source": [
    "batch_size=200\n",
    "B=2000\n",
    "params[\"n_conv_layers\"]=1\n",
    "\n",
    "ml = []\n",
    "ml2 = {}\n",
    "\n",
    "# ppi_pair_id=\"Sim_1_traj_42_frame_1307\"\n",
    "for i, ppi_pair_id in enumerate(ppi_pair_ids[0:B]):\n",
    "    try:\n",
    "        if i % 50 == 0:\n",
    "            print(i)\n",
    "        mydir=data+\"/data_preparation/04a-precomputation_12A/precomputation/\" + ppi_pair_id + \"/\"\n",
    "        pdbid = ppi_pair_id.split(\".\")[0]\n",
    "        pid=pdbid\n",
    "        rho_wrt_center = np.load(mydir + pid + \"_rho_wrt_center.npy\")\n",
    "        theta_wrt_center = np.load(mydir + pid + \"_theta_wrt_center.npy\")\n",
    "        input_feat = np.load(mydir + pid + \"_input_feat.npy\")\n",
    "\n",
    "        iface_labels = np.load(mydir + pid + \"_iface_labels.npy\")\n",
    "\n",
    "        mask = np.load(mydir + pid + \"_mask.npy\")\n",
    "        mask = np.expand_dims(mask, 2)\n",
    "        indices = np.load(mydir + pid + \"_list_indices.npy\", allow_pickle=True).item()['protein']\n",
    "\n",
    "        # indices is (n_verts x <30), it should be\n",
    "        # indices = pad_indices(indices, mask.shape[1])\n",
    "        tmp = np.zeros((len(iface_labels), 3))\n",
    "        for i in range(len(iface_labels)):\n",
    "            if iface_labels[i] == 1:\n",
    "                tmp[i, 1] = 1\n",
    "            elif iface_labels[i] == 2:\n",
    "                tmp[i, 2] = 1\n",
    "            else:\n",
    "                tmp[i, 0] = 1\n",
    "        iface_labels_dc = tmp\n",
    "        super_pos = np.where(iface_labels == 2)[0]\n",
    "        pos_labels = np.where(iface_labels == 1)[0]\n",
    "        neg_labels = np.where(iface_labels == 0)[0]\n",
    "        # print(\"PNS\",len(pos_labels),len(neg_labels), len(super_pos))\n",
    "\n",
    "\n",
    "        n = min(len(pos_labels), len(neg_labels))\n",
    "        # print(ppi_pair_id,\"len(pos_labels)\", len(pos_labels), 'len(super_pos)', len(super_pos),'len(neg_labels)', len(neg_labels))\n",
    "        n = min(n, batch_size // 2)\n",
    "        n_neg = 500-n-len(super_pos)\n",
    "        # this is \n",
    "        subset = np.concatenate([neg_labels[:n_neg], pos_labels[:n], super_pos])\n",
    "        # print(len(subset))\n",
    "\n",
    "        rho_wrt_center = rho_wrt_center[subset]\n",
    "        theta_wrt_center = theta_wrt_center[subset]\n",
    "        input_feat = input_feat[subset]\n",
    "        mask = mask[subset]\n",
    "        iface_labels_dc = iface_labels_dc[subset]\n",
    "        indices = indices[subset]\n",
    "        # neg_labels = range(0, (n*5))\n",
    "        # pos_labels = range((n*5), (n*5)+n)\n",
    "        # super_pos_labels = range(n+(n*5),n+(n*5)+len(super_pos))\n",
    "        neg_labels = range(0, n_neg)\n",
    "        pos_labels = range(n_neg, n_neg+n)\n",
    "        super_pos_labels = range(n+(n_neg),n+(n_neg)+len(super_pos))\n",
    "        # print(\"PNS labels\",len(neg_labels),len(pos_labels), len(super_pos_labels))\n",
    "        # print(ppi_pair_id,\"len(pos_labels)\", len(pos_labels[:n]), 'len(super_pos_labels)', len(super_pos_labels),'len(neg_labels)', len(neg_labels[:n_neg]), len(np.concatenate([neg_labels,pos_labels,super_pos_labels])))\n",
    "        assert(len(subset) == 500)\n",
    "\n",
    "        ml.append([input_feat,rho_wrt_center,theta_wrt_center,mask,iface_labels_dc])\n",
    "        ml2[ppi_pair_id] = {\n",
    "            'input_feat':input_feat,\n",
    "            'rho_wrt_center':rho_wrt_center,\n",
    "            'theta_wrt_center':theta_wrt_center,\n",
    "            'mask':mask,\n",
    "            'iface_labels_dc':iface_labels_dc\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(\"Error\", e, ppi_pair_id)\n",
    "\n",
    "np.save(data+\"/inputs_B%d.npy\"%B, ml2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(data+\"/inputs_B%d.npy\"%B, ml2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ml2 = np.load(\"inputs_B%d.npy\"%B, allow_pickle=True).item()\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"inputs_B%d.npy\"%B, ml2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "6000/B*769/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import tensorflow.keras as keras\n",
    "\n",
    "\n",
    "# class MyModel(keras.Model):\n",
    "#   def __init__(self, n_rotations=1):\n",
    "#     super(MyModel, self).__init__()\n",
    "#     self.n_rotations=n_rotations,\n",
    "#     self.conv1 = MaSIF_site2(\n",
    "#         params[\"max_distance\"],\n",
    "#         n_thetas=4,\n",
    "#         n_rhos=3,\n",
    "#         n_rotations=self.n_rotations,\n",
    "#         idx_gpu=\"/gpu:1\",\n",
    "#         feat_mask=[1.0]*5,\n",
    "#         n_conv_layers=params[\"n_conv_layers\"],\n",
    "#     )\n",
    "\n",
    "#   def call(self, inputs):\n",
    "#     logits = self.conv1(inputs)\n",
    "#     return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_TEST_SPLIT = 0.7\n",
    "\n",
    "class SimDataGenerator():\n",
    "    \"\"\"\n",
    "    Data generator for the UTKFace dataset. This class should be used when training our Keras multi-output model.\n",
    "    \"\"\"\n",
    "    def __init__(self, list_):\n",
    "        self.list_ = list_\n",
    "        \n",
    "    def generate_split_indexes(self):\n",
    "        p = np.random.permutation(len(self.list_))\n",
    "        train_up_to = int(len(self.list_) * TRAIN_TEST_SPLIT)\n",
    "        train_idx = p[:train_up_to]\n",
    "        test_idx = p[train_up_to:]\n",
    "\n",
    "        train_up_to = int(train_up_to * TRAIN_TEST_SPLIT)\n",
    "        train_idx, valid_idx = train_idx[:train_up_to], train_idx[train_up_to:]\n",
    "        \n",
    "        # # converts alias to id\n",
    "        # self.df['gender_id'] = self.df['gender'].map(lambda gender: dataset_dict['gender_alias'][gender])\n",
    "        # self.df['race_id'] = self.df['race'].map(lambda race: dataset_dict['race_alias'][race])\n",
    "\n",
    "        # self.max_age = self.df['age'].max()\n",
    "        \n",
    "        return train_idx, valid_idx, test_idx\n",
    "\n",
    "    def generate_frames(self, frame_idx, is_training, batch_size=16):\n",
    "        \"\"\"\n",
    "        Used to generate a batch with images when training/testing/validating our Keras model.\n",
    "        \"\"\"\n",
    "        \n",
    "        # arrays to store our batched data\n",
    "        input_feats, rhos, thetas, masks, labels = [], [], [], [], []\n",
    "        while True:\n",
    "            for idx in frame_idx:\n",
    "                sample = self.list_[idx]\n",
    "                \n",
    "                input_feat = sample[0]\n",
    "                rho = sample[1]\n",
    "                theta = sample[2]\n",
    "                mask = sample[3]\n",
    "                label = sample[4]\n",
    "                \n",
    "                input_feats.append(input_feat)\n",
    "                rhos.append(rho)\n",
    "                thetas.append(theta)\n",
    "                masks.append(mask)\n",
    "                labels.append(label)\n",
    "                \n",
    "                # yielding condition\n",
    "                if len(input_feats) >= batch_size:\n",
    "                    # print(\"True\")\n",
    "                    yield (np.array(input_feats), np.array(rhos), np.array(thetas), np.array(masks)), np.array(labels)\n",
    "                    input_feats, rhos, thetas, masks, labels = [], [], [], [], []\n",
    "                    \n",
    "            if not is_training:\n",
    "                break\n",
    "                \n",
    "data_generator = SimDataGenerator(ml)\n",
    "train_idx, valid_idx, test_idx = data_generator.generate_split_indexes()\n",
    "\n",
    "batch_size = 32\n",
    "valid_batch_size = 32\n",
    "train_gen = data_generator.generate_frames(train_idx, is_training=True, batch_size=batch_size)\n",
    "valid_gen = data_generator.generate_frames(valid_idx, is_training=True, batch_size=valid_batch_size)\n",
    "test_gen = data_generator.generate_frames(test_idx, is_training=False, batch_size=valid_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaSIF_site2(tf.keras.layers.Layer):\n",
    "\n",
    "    def inference(\n",
    "        self,\n",
    "        input_feat, # learning_obj.input_feat\n",
    "        rho_coords, # learning_obj.rho_coords\n",
    "        theta_coords,\n",
    "        mask,\n",
    "        W_conv,\n",
    "        b_conv,\n",
    "        mu_rho,\n",
    "        sigma_rho,\n",
    "        mu_theta,\n",
    "        sigma_theta,\n",
    "        eps=1e-5,\n",
    "        mean_gauss_activation=True,\n",
    "    ):\n",
    "        n_samples = rho_coords.shape[1]\n",
    "        n_vertices = rho_coords.shape[2]\n",
    "        # print(\"n_samples\", rho_coords.shape)\n",
    "        # print(\"vertices\", n_vertices)\n",
    "\n",
    "        all_conv_feat = []\n",
    "        for k in range(self.n_rotations):\n",
    "            # print(\"rotation\", k)\n",
    "            # print(\"\\trotation\", k+1)\n",
    "            # rho_coords_ = tf.reshape(rho_coords, [-1, 1])  # batch_size*n_vertices\n",
    "            rho_coords_ = tf.reshape(rho_coords, (-1,n_samples*n_vertices, 1))\n",
    "            # print(\"Brian rho_coords,rho_coords_\", rho_coords.shape, rho_coords_.shape)\n",
    "            # thetas_coords_ = tf.reshape(theta_coords, [-1, 1])  # batch_size*n_vertices\n",
    "            thetas_coords_ = tf.reshape(theta_coords, (-1,n_samples*n_vertices,1))\n",
    "            # print(\"thetas_coords,thetas_coords_\", theta_coords.shape, thetas_coords_.shape)\n",
    "            thetas_coords_ += k * 2 * np.pi / self.n_rotations\n",
    "            thetas_coords_ = tf.math.mod(thetas_coords_, 2 * np.pi)\n",
    "            rho_coords_ = tf.exp(\n",
    "                -tf.square(rho_coords_ - mu_rho) / (tf.square(sigma_rho) + eps)\n",
    "            )\n",
    "            thetas_coords_ = tf.exp(\n",
    "                -tf.square(thetas_coords_ - mu_theta) / (tf.square(sigma_theta) + eps)\n",
    "            )\n",
    "            \n",
    "            gauss_activations = tf.multiply(\n",
    "                rho_coords_, thetas_coords_\n",
    "            )  # batch_size*n_vertices, n_gauss\n",
    "            # print(\"gauss_activations shape 1\",gauss_activations.shape)\n",
    "            # print(\"gauss_activations1\", gauss_activations.shape, rho_coords_.shape, thetas_coords_.shape)\n",
    "            gauss_activations = tf.reshape(\n",
    "                # gauss_activations, [n_samples, n_vertices, -1]\n",
    "                gauss_activations,[-1,n_samples, n_vertices ,tf.shape(gauss_activations)[-1]]\n",
    "            )  # batch_size, n_vertices, n_gauss\n",
    "            # print(\"gauss_activations2\", gauss_activations.shape, n_samples, n_vertices,-1)\n",
    "            # print(\"gauss_activations shape 2\",gauss_activations.shape)\n",
    "            # print(\"gauss_activations,mask\", gauss_activations.shape, mask.shape)\n",
    "            gauss_activations = tf.multiply(gauss_activations, mask)\n",
    "            # print(\"this is actually working\")\n",
    "            \n",
    "            if (\n",
    "                mean_gauss_activation\n",
    "            ):  # computes mean weights for the different gaussians\n",
    "                gauss_activations /= (\n",
    "                    tf.reduce_sum(gauss_activations, 1, keepdims=True) + eps\n",
    "                )  # batch_size, n_vertices, n_gauss\n",
    "            # print(\"1\", gauss_activations.shape)\n",
    "            gauss_activations = tf.expand_dims(\n",
    "                # gauss_activations, 2\n",
    "                gauss_activations, 3\n",
    "            )  # batch_size, inputsize, n_vertices, 1, n_gauss,\n",
    "            # print(\"2\", gauss_activations.shape)\n",
    "            input_feat_ = tf.expand_dims(\n",
    "                # input_feat, 3\n",
    "                input_feat, 4\n",
    "            )  # batch_size, inputsize, n_vertices, n_feat, 1\n",
    "            # print(\"3\", input_feat_.shape)\n",
    "            gauss_desc = tf.multiply(\n",
    "                gauss_activations, input_feat_\n",
    "            )  # batch_size, n_vertices, n_feat, n_gauss,\n",
    "            # print(\"4\", gauss_desc.shape)\n",
    "            gauss_desc = tf.reduce_sum(gauss_desc, 2)  # batch_size, n_feat, n_gauss,\n",
    "            # print(\"5\", gauss_desc.shape)\n",
    "            gauss_desc = tf.reshape(\n",
    "                gauss_desc, [-1,n_samples, self.n_thetas * self.n_rhos]\n",
    "            )  # batch_size, 80\n",
    "            \n",
    "            # print(\"6\", gauss_desc.shape)\n",
    "            conv_feat = tf.matmul(gauss_desc, W_conv) + b_conv  # batch_size, 80\n",
    "            # print(\"7\", conv_feat.shape)\n",
    "            # rho_coords2 = tf.identity(conv_feat)\n",
    "            all_conv_feat.append(conv_feat)\n",
    "\n",
    "        all_conv_feat = tf.stack(all_conv_feat)\n",
    "        conv_feat = tf.reduce_max(all_conv_feat, 0)\n",
    "        conv_feat = tf.nn.relu(conv_feat)\n",
    "        return conv_feat\n",
    "        # return 1,2\n",
    "        \n",
    "    def __init__(\n",
    "        self,\n",
    "        max_rho,\n",
    "        n_thetas=4,\n",
    "        n_rhos=3,\n",
    "        n_gamma=1.0,\n",
    "        learning_rate=1e-3,\n",
    "        n_rotations=1,\n",
    "        idx_gpu=\"/device:GPU:0\",\n",
    "        feat_mask=[1.0, 1.0, 1.0, 1.0, 1.0],\n",
    "        n_conv_layers=1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.max_rho=max_rho\n",
    "       \n",
    "        self.n_rhos=n_rhos\n",
    "        self.n_thetas=n_thetas\n",
    "        self.sigma_rho_init = (\n",
    "            max_rho / 8\n",
    "        ) \n",
    "        self.sigma_theta_init = 1.0\n",
    "        self.learning_rate=learning_rate\n",
    "        self.n_rotations=n_rotations\n",
    "        self.n_feat = int(sum(feat_mask))\n",
    "        self.n_labels = 3\n",
    "\n",
    "        initial_coords = self.compute_initial_coordinates()\n",
    "        self.initial_coords = initial_coords\n",
    "        mu_rho_initial = np.expand_dims(initial_coords[:, 0], 0).astype(\n",
    "                    \"float32\"\n",
    "                )\n",
    "        mu_theta_initial = np.expand_dims(initial_coords[:, 1], 0).astype(\n",
    "            \"float32\"\n",
    "        )\n",
    "        # print(mu_rho_initial[0:100])\n",
    "        self.mu_rho = []\n",
    "        self.mu_theta = []\n",
    "        self.sigma_rho = []\n",
    "        self.sigma_theta = []\n",
    "        for i in range(self.n_feat):\n",
    "            self.mu_rho.append(\n",
    "                tf.Variable(mu_rho_initial, name=\"mu_rho_{}\".format(i))\n",
    "            )  # 1, n_gauss\n",
    "            self.mu_theta.append(\n",
    "                tf.Variable(mu_theta_initial, name=\"mu_theta_{}\".format(i))\n",
    "            )  # 1, n_gauss\n",
    "            self.sigma_rho.append(\n",
    "                tf.Variable(\n",
    "                    np.ones_like(mu_rho_initial) * self.sigma_rho_init,\n",
    "                    name=\"sigma_rho_{}\".format(i),\n",
    "                )\n",
    "            )  # 1, n_gauss\n",
    "            self.sigma_theta.append(\n",
    "                tf.Variable(\n",
    "                    (np.ones_like(mu_theta_initial) * self.sigma_theta_init),\n",
    "                    name=\"sigma_theta_{}\".format(i),\n",
    "                )\n",
    "            )  # 1, n_gauss\n",
    "        \n",
    "        \n",
    "        self.b_conv = []\n",
    "        for i in range(self.n_feat):\n",
    "            self.b_conv.append(\n",
    "                tf.Variable(\n",
    "                    tf.zeros([self.n_thetas * self.n_rhos]),\n",
    "                    name=\"b_conv_{}\".format(i),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.W_conv = []\n",
    "        \n",
    "        for i in range(self.n_feat):\n",
    "            initializer=tf.keras.initializers.GlorotUniform(seed=(i+1)*100)\n",
    "            self.W_conv.append(\n",
    "                    tf.Variable(initializer((self.n_thetas * self.n_rhos, \n",
    "                                                self.n_thetas * self.n_rhos)),\n",
    "                                name=\"W_conv_{}\".format(i)\n",
    "                                )\n",
    "            )\n",
    "        self.mlp2= tf.keras.layers.Dense(\n",
    "                    self.n_thetas * self.n_rhos,\n",
    "                    activation=tf.nn.relu,\n",
    "                )\n",
    "        self.mlp3= tf.keras.layers.Dense(\n",
    "                    self.n_feat,\n",
    "                    activation=tf.nn.relu,\n",
    "                )\n",
    "        self.mlp4= tf.keras.layers.Dense(\n",
    "                    self.n_labels, activation='softmax'\n",
    "                )\n",
    "              \n",
    "        # self.b = self.add_weight(shape=(self.n_ligands*,), initializer=\"zeros\", trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # print(\"INPUTS!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\", len(inputs))\n",
    "        input_feat=inputs[0]\n",
    "        print(\"\\tinput_feat\", input_feat.shape)\n",
    "        rho_coords=inputs[1]\n",
    "        # print(\"\\trho_coords\", rho_coords.shape)\n",
    "        theta_coords=inputs[2]\n",
    "        # print(\"\\ttheta_coords\", theta_coords.shape)\n",
    "        mask=inputs[3]\n",
    "        # print(\"\\tmask\", mask.shape)\n",
    "        # self.keep_prob=inputs[4]\n",
    "        # labels=inputs[5]\n",
    "    \n",
    "        self.global_desc_1 = []\n",
    "        self.rho_coords2 = []\n",
    "        for i in range(self.n_feat):\n",
    "        # for i in range(1):\n",
    "            # print('feature',['si', 'ddc', 'hbond', 'charge', 'hphob'][i])\n",
    "            # my_input_feat = tf.keras.ops.expand_dims(input_feat[:, :, i], 2)\n",
    "            my_input_feat = tf.keras.ops.expand_dims(input_feat[:, :, :, i], 3)\n",
    "            # print(\"WHAT\", my_input_feat.shape)\n",
    "            conv_feat = self.inference(\n",
    "                            my_input_feat,\n",
    "                            rho_coords,\n",
    "                            theta_coords,\n",
    "                            mask,\n",
    "                            self.W_conv[i],\n",
    "                            self.b_conv[i],\n",
    "                            self.mu_rho[i],\n",
    "                            self.sigma_rho[i],\n",
    "                            self.mu_theta[i],\n",
    "                            self.sigma_theta[i],\n",
    "                        )\n",
    "            self.global_desc_1.append(conv_feat)\n",
    "\n",
    "        self.global_desc_1 = tf.stack(self.global_desc_1, axis=2)  \n",
    "        # print(\"8\", self.global_desc_1.shape)\n",
    "\n",
    "        self.global_desc_1 = tf.reshape(\n",
    "                    self.global_desc_1, [-1,500, self.n_thetas * self.n_rhos * self.n_feat]\n",
    "                )\n",
    "        # print(\"9\", self.global_desc_1.shape)\n",
    "\n",
    "        self.global_desc_1=self.mlp2(self.global_desc_1)\n",
    "        # print(\"10\", self.global_desc_1.shape)\n",
    "\n",
    "        self.global_desc_1=self.mlp3(self.global_desc_1)\n",
    "        # print(\"11\", self.global_desc_1.shape)\n",
    "       \n",
    "        self.logits=self.mlp4(self.global_desc_1)\n",
    "        # print(\"shape logits\", self.logits.shape)\n",
    "       \n",
    "        # return self.global_desc_2, self.logits, self.rho_coords2\n",
    "        return self.logits\n",
    "\n",
    "\n",
    "    def compute_initial_coordinates(self):\n",
    "        range_rho = [0.0, self.max_rho]\n",
    "        range_theta = [0, 2 * np.pi]\n",
    "\n",
    "        grid_rho = np.linspace(range_rho[0], range_rho[1], num=self.n_rhos + 1)\n",
    "        grid_rho = grid_rho[1:]\n",
    "        grid_theta = np.linspace(range_theta[0], range_theta[1], num=self.n_thetas + 1)\n",
    "        grid_theta = grid_theta[:-1]\n",
    "\n",
    "        grid_rho_, grid_theta_ = np.meshgrid(grid_rho, grid_theta, sparse=False)\n",
    "        grid_rho_ = (\n",
    "            grid_rho_.T\n",
    "        )  # the traspose here is needed to have the same behaviour as Matlab code\n",
    "        grid_theta_ = (\n",
    "            grid_theta_.T\n",
    "        )  # the traspose here is needed to have the same behaviour as Matlab code\n",
    "        grid_rho_ = grid_rho_.flatten()\n",
    "        grid_theta_ = grid_theta_.flatten()\n",
    "\n",
    "        coords = np.concatenate((grid_rho_[None, :], grid_theta_[None, :]), axis=0)\n",
    "        coords = coords.T  # every row contains the coordinates of a grid intersection\n",
    "        # print(coords.shape)\n",
    "        return coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "\n",
    "class MyModel(keras.Model):\n",
    "  def __init__(self, n_rotations=1):\n",
    "    super(MyModel, self).__init__()\n",
    "    self.n_rotations=n_rotations\n",
    "    print(\"self.n_rotations\", self.n_rotations)\n",
    "    self.conv1 = MaSIF_site2(\n",
    "        params[\"max_distance\"],\n",
    "        n_thetas=4,\n",
    "        n_rhos=3,\n",
    "        n_rotations=self.n_rotations,\n",
    "        idx_gpu=\"/gpu:1\",\n",
    "        feat_mask=[1.0]*5,\n",
    "        n_conv_layers=params[\"n_conv_layers\"],\n",
    "    )\n",
    "\n",
    "  def call(self, inputs):\n",
    "    logits = self.conv1(inputs)\n",
    "    return logits\n",
    "\n",
    "# inputA=keras.layers.Input((500,50,5))\n",
    "# inputB=keras.layers.Input((500,50))\n",
    "# inputC=keras.layers.Input((500,50))\n",
    "# inputD=keras.layers.Input((500,50,1))\n",
    "\n",
    "# model = MyModel(n_rotations=8)\n",
    "# model([inputA,inputB,inputC,inputD])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_idx)//batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputA=keras.layers.Input((500,50,5))\n",
    "# inputB=keras.layers.Input((500,50))\n",
    "# inputC=keras.layers.Input((500,50))\n",
    "# inputD=keras.layers.Input((500,50,1))\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "model = MyModel(n_rotations=4)\n",
    "# model([inputA,inputB,inputC,inputD])\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\"./model_checkpoint.keras\", monitor='val_loss')\n",
    "]\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate= 1e-3), \n",
    "    loss='categorical_crossentropy', \n",
    "    metrics = ['accuracy'])\n",
    "\n",
    "model.fit(train_gen, steps_per_epoch=len(train_idx)//batch_size, epochs=5, callbacks=callbacks,\n",
    "                    validation_data=valid_gen,\n",
    "                    validation_steps=len(valid_idx)//valid_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MaSIF_site2(tf.keras.layers.Layer):\n",
    "\n",
    "#     def inference(\n",
    "#         self,\n",
    "#         input_feat, # learning_obj.input_feat\n",
    "#         rho_coords, # learning_obj.rho_coords\n",
    "#         theta_coords,\n",
    "#         mask,\n",
    "#         W_conv,\n",
    "#         b_conv,\n",
    "#         mu_rho,\n",
    "#         sigma_rho,\n",
    "#         mu_theta,\n",
    "#         sigma_theta,\n",
    "#         eps=1e-5,\n",
    "#         mean_gauss_activation=True,\n",
    "#     ):\n",
    "#         n_samples = tf.shape(rho_coords)[0]\n",
    "#         n_vertices = tf.shape(rho_coords)[1]\n",
    "#         print(\"n_samples\",tf.shape(rho_coords))\n",
    "#         print(\"vertices\",n_vertices)\n",
    "\n",
    "#         all_conv_feat = []\n",
    "#         for k in range(self.n_rotations):\n",
    "#             # print(\"\\trotation\", k+1)\n",
    "#             rho_coords_ = tf.reshape(rho_coords, [-1, 1])  # batch_size*n_vertices\n",
    "#             # rho_coords_ = tf.reshape(rho_coords, (-1,500*50, 1))\n",
    "#             print(\"Brian rho_coords,rho_coords_\", rho_coords.shape, rho_coords_.shape)\n",
    "#             thetas_coords_ = tf.reshape(theta_coords, [-1, 1])  # batch_size*n_vertices\n",
    "#             # thetas_coords_ = tf.reshape(theta_coords, (-1,500*50,1))\n",
    "#             print(\"thetas_coords,thetas_coords_\", theta_coords.shape, thetas_coords_.shape)\n",
    "#             thetas_coords_ += k * 2 * np.pi / self.n_rotations\n",
    "#             thetas_coords_ = tf.math.mod(thetas_coords_, 2 * np.pi)\n",
    "#             rho_coords_ = tf.exp(\n",
    "#                 -tf.square(rho_coords_ - mu_rho) / (tf.square(sigma_rho) + eps)\n",
    "#             )\n",
    "#             thetas_coords_ = tf.exp(\n",
    "#                 -tf.square(thetas_coords_ - mu_theta) / (tf.square(sigma_theta) + eps)\n",
    "#             )\n",
    "            \n",
    "#             gauss_activations = tf.multiply(\n",
    "#                 rho_coords_, thetas_coords_\n",
    "#             )  # batch_size*n_vertices, n_gauss\n",
    "#             # print(\"gauss_activations shape 1\",gauss_activations.shape)\n",
    "#             print(\"gauss_activations1\", gauss_activations.shape, rho_coords_.shape, thetas_coords_.shape)\n",
    "#             gauss_activations = tf.reshape(\n",
    "#                 gauss_activations, [n_samples, n_vertices, -1]\n",
    "#                 # gauss_activations,[-1,500, 50,tf.shape(gauss_activations)[-1]]\n",
    "#             )  # batch_size, n_vertices, n_gauss\n",
    "#             # print(\"gauss_activations2\", gauss_activations.shape, n_samples, n_vertices,-1)\n",
    "#             # print(\"gauss_activations shape 2\",gauss_activations.shape)\n",
    "#             print(\"gauss_activations,mask\", gauss_activations.shape, mask.shape)\n",
    "#             gauss_activations = tf.multiply(gauss_activations, mask)\n",
    "#             print(\"this is actually working\")\n",
    "            \n",
    "#             if (\n",
    "#                 mean_gauss_activation\n",
    "#             ):  # computes mean weights for the different gaussians\n",
    "#                 gauss_activations /= (\n",
    "#                     tf.reduce_sum(gauss_activations, 1, keepdims=True) + eps\n",
    "#                 )  # batch_size, n_vertices, n_gauss\n",
    "#             print(\"1\", gauss_activations.shape)\n",
    "#             gauss_activations = tf.expand_dims(\n",
    "#                 gauss_activations, 2\n",
    "#                 # gauss_activations, 3\n",
    "#             )  # batch_size, inputsize, n_vertices, 1, n_gauss,\n",
    "#             print(\"2\", gauss_activations.shape)\n",
    "#             input_feat_ = tf.expand_dims(\n",
    "#                 input_feat, 3\n",
    "#                 # input_feat, 4\n",
    "#             )  # batch_size, inputsize, n_vertices, n_feat, 1\n",
    "#             print(\"3\", input_feat_.shape)\n",
    "#             gauss_desc = tf.multiply(\n",
    "#                 gauss_activations, input_feat_\n",
    "#             )  # batch_size, n_vertices, n_feat, n_gauss,\n",
    "#             print(\"4\", gauss_desc.shape)\n",
    "#             gauss_desc = tf.reduce_sum(gauss_desc, 1)  # batch_size, n_feat, n_gauss,\n",
    "#             print(\"5\", gauss_desc.shape)\n",
    "#             gauss_desc = tf.reshape(\n",
    "#                 gauss_desc, [n_samples, self.n_thetas * self.n_rhos]\n",
    "#             )  # batch_size, 80\n",
    "            \n",
    "#             print(\"6\", gauss_desc.shape)\n",
    "#             conv_feat = tf.matmul(gauss_desc, W_conv) + b_conv  # batch_size, 80\n",
    "#             print(\"7\", conv_feat.shape)\n",
    "#             rho_coords2 = tf.identity(conv_feat)\n",
    "#             all_conv_feat.append(conv_feat)\n",
    "\n",
    "#         all_conv_feat = tf.stack(all_conv_feat)\n",
    "#         conv_feat = tf.reduce_max(all_conv_feat, 0)\n",
    "#         conv_feat = tf.nn.relu(conv_feat)\n",
    "#         return conv_feat, rho_coords2\n",
    "#         # return 1,2\n",
    "        \n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         max_rho,\n",
    "#         n_thetas=4,\n",
    "#         n_rhos=3,\n",
    "#         n_gamma=1.0,\n",
    "#         learning_rate=1e-3,\n",
    "#         n_rotations=1,\n",
    "#         idx_gpu=\"/device:GPU:0\",\n",
    "#         feat_mask=[1.0, 1.0, 1.0, 1.0, 1.0],\n",
    "#         n_conv_layers=1,\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.max_rho=max_rho\n",
    "       \n",
    "#         self.n_rhos=n_rhos\n",
    "#         self.n_thetas=n_thetas\n",
    "#         self.sigma_rho_init = (\n",
    "#             max_rho / 8\n",
    "#         ) \n",
    "#         self.sigma_theta_init = 1.0\n",
    "#         self.learning_rate=learning_rate\n",
    "#         self.n_rotations=n_rotations\n",
    "#         self.n_feat = int(sum(feat_mask))\n",
    "#         self.n_labels = 3\n",
    "\n",
    "#         initial_coords = self.compute_initial_coordinates()\n",
    "#         self.initial_coords = initial_coords\n",
    "#         mu_rho_initial = np.expand_dims(initial_coords[:, 0], 0).astype(\n",
    "#                     \"float32\"\n",
    "#                 )\n",
    "#         mu_theta_initial = np.expand_dims(initial_coords[:, 1], 0).astype(\n",
    "#             \"float32\"\n",
    "#         )\n",
    "#         # print(mu_rho_initial[0:100])\n",
    "#         self.mu_rho = []\n",
    "#         self.mu_theta = []\n",
    "#         self.sigma_rho = []\n",
    "#         self.sigma_theta = []\n",
    "#         for i in range(self.n_feat):\n",
    "#             self.mu_rho.append(\n",
    "#                 tf.Variable(mu_rho_initial, name=\"mu_rho_{}\".format(i))\n",
    "#             )  # 1, n_gauss\n",
    "#             self.mu_theta.append(\n",
    "#                 tf.Variable(mu_theta_initial, name=\"mu_theta_{}\".format(i))\n",
    "#             )  # 1, n_gauss\n",
    "#             self.sigma_rho.append(\n",
    "#                 tf.Variable(\n",
    "#                     np.ones_like(mu_rho_initial) * self.sigma_rho_init,\n",
    "#                     name=\"sigma_rho_{}\".format(i),\n",
    "#                 )\n",
    "#             )  # 1, n_gauss\n",
    "#             self.sigma_theta.append(\n",
    "#                 tf.Variable(\n",
    "#                     (np.ones_like(mu_theta_initial) * self.sigma_theta_init),\n",
    "#                     name=\"sigma_theta_{}\".format(i),\n",
    "#                 )\n",
    "#             )  # 1, n_gauss\n",
    "        \n",
    "        \n",
    "#         self.b_conv = []\n",
    "#         for i in range(self.n_feat):\n",
    "#             self.b_conv.append(\n",
    "#                 tf.Variable(\n",
    "#                     tf.zeros([self.n_thetas * self.n_rhos]),\n",
    "#                     name=\"b_conv_{}\".format(i),\n",
    "#                 )\n",
    "#             )\n",
    "\n",
    "#         self.W_conv = []\n",
    "#         initializer=tf.keras.initializers.glorot_normal(seed=0)\n",
    "#         for i in range(self.n_feat):\n",
    "#             self.W_conv.append(\n",
    "#                     tf.Variable(glorot_uniform((self.n_thetas * self.n_rhos, \n",
    "#                                                 self.n_thetas * self.n_rhos),\n",
    "#                                             seed=(i+1)*100),\n",
    "#                                 name=\"W_conv_{}\".format(i)\n",
    "#                                 )\n",
    "#             )\n",
    "#         self.mlp2= tf.keras.layers.Dense(\n",
    "#                     self.n_thetas * self.n_rhos,\n",
    "#                     activation=tf.nn.relu,\n",
    "#                 )\n",
    "#         self.mlp3= tf.keras.layers.Dense(\n",
    "#                     self.n_feat,\n",
    "#                     activation=tf.nn.relu,\n",
    "#                 )\n",
    "#         self.mlp4= tf.keras.layers.Dense(\n",
    "#                     self.n_labels, activation='softmax'\n",
    "#                 )\n",
    "              \n",
    "#         # self.b = self.add_weight(shape=(self.n_ligands*,), initializer=\"zeros\", trainable=True)\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         print(\"INPUTS!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\", len(inputs))\n",
    "#         input_feat=inputs[0]\n",
    "#         print(\"\\tinput_feat\", input_feat.shape)\n",
    "#         rho_coords=inputs[1]\n",
    "#         print(\"\\trho_coords\", rho_coords.shape)\n",
    "#         theta_coords=inputs[2]\n",
    "#         print(\"\\ttheta_coords\", theta_coords.shape)\n",
    "#         mask=inputs[3]\n",
    "#         print(\"\\tmask\", mask.shape)\n",
    "#         # self.keep_prob=inputs[4]\n",
    "#         # labels=inputs[5]\n",
    "    \n",
    "#         self.global_desc_1 = []\n",
    "#         self.rho_coords2 = []\n",
    "#         for i in range(self.n_feat):\n",
    "#         # for i in range(1):\n",
    "#             # print('feature',['si', 'ddc', 'hbond', 'charge', 'hphob'][i])\n",
    "#             my_input_feat = tf.keras.ops.expand_dims(input_feat[:, :, i], 2)\n",
    "#             # my_input_feat = tf.keras.ops.expand_dims(input_feat[:, :, :, i], 3)\n",
    "#             print(\"WHAT\", my_input_feat.shape)\n",
    "#             conv_feat, rho_coords2, = self.inference(\n",
    "#                             my_input_feat,\n",
    "#                             rho_coords,\n",
    "#                             theta_coords,\n",
    "#                             mask,\n",
    "#                             self.W_conv[i],\n",
    "#                             self.b_conv[i],\n",
    "#                             self.mu_rho[i],\n",
    "#                             self.sigma_rho[i],\n",
    "#                             self.mu_theta[i],\n",
    "#                             self.sigma_theta[i],\n",
    "#                         )\n",
    "#             self.global_desc_1.append(conv_feat)\n",
    "            \n",
    "\n",
    "#         self.global_desc_1 = tf.stack(self.global_desc_1, axis=1)\n",
    "#         print(\"8\", self.global_desc_1.shape)\n",
    "#         self.global_desc_1 = tf.reshape(\n",
    "#                     self.global_desc_1, [-1, self.n_thetas * self.n_rhos * self.n_feat]\n",
    "#                 )\n",
    "#         print(\"9\", self.global_desc_1.shape)\n",
    "#         self.global_desc_1=self.mlp2(self.global_desc_1)\n",
    "#         # self.global_desc_1 = tf.keras.layers.Dense(\n",
    "#         #             self.n_thetas * self.n_rhos,\n",
    "#         #             activation=tf.nn.relu,\n",
    "#         #         )(self.global_desc_1)\n",
    "#         print(\"10\", self.global_desc_1.shape)\n",
    "#         self.global_desc_1=self.mlp3(self.global_desc_1)\n",
    "#         print(\"11\", self.global_desc_1.shape)\n",
    "#         # self.global_desc_1 = tf.keras.layers.Dense(\n",
    "#         #             self.n_feat, activation=tf.nn.relu\n",
    "#         #         )(self.global_desc_1)\n",
    "#         self.logits=self.mlp4(self.global_desc_1)\n",
    "#         print(\"shape logits\", self.logits.shape)\n",
    "#         # self.logits = tf.keras.layers.Dense(\n",
    "#         #             self.n_labels, activation='softmax'\n",
    "#         # )(self.global_desc_1)\n",
    "        \n",
    "#         # return self.global_desc_2, self.logits, self.rho_coords2\n",
    "#         return self.logits\n",
    "\n",
    "    \n",
    "#     def compute_initial_coordinates(self):\n",
    "#         range_rho = [0.0, self.max_rho]\n",
    "#         range_theta = [0, 2 * np.pi]\n",
    "\n",
    "#         grid_rho = np.linspace(range_rho[0], range_rho[1], num=self.n_rhos + 1)\n",
    "#         grid_rho = grid_rho[1:]\n",
    "#         grid_theta = np.linspace(range_theta[0], range_theta[1], num=self.n_thetas + 1)\n",
    "#         grid_theta = grid_theta[:-1]\n",
    "\n",
    "#         grid_rho_, grid_theta_ = np.meshgrid(grid_rho, grid_theta, sparse=False)\n",
    "#         grid_rho_ = (\n",
    "#             grid_rho_.T\n",
    "#         )  # the traspose here is needed to have the same behaviour as Matlab code\n",
    "#         grid_theta_ = (\n",
    "#             grid_theta_.T\n",
    "#         )  # the traspose here is needed to have the same behaviour as Matlab code\n",
    "#         grid_rho_ = grid_rho_.flatten()\n",
    "#         grid_theta_ = grid_theta_.flatten()\n",
    "\n",
    "#         coords = np.concatenate((grid_rho_[None, :], grid_theta_[None, :]), axis=0)\n",
    "#         coords = coords.T  # every row contains the coordinates of a grid intersection\n",
    "#         print(coords.shape)\n",
    "#         return coords\n",
    "\n",
    "# model = MyModel()\n",
    "# logits = model([input_feat,rho,theta,mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gen = data_generator.generate_frames(test_idx, is_training=False, batch_size=valid_batch_size)\n",
    "\n",
    "model.evaluate(test_gen, steps=len(test_idx)//valid_batch_size, \n",
    "               callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaSIF_site2(tf.keras.layers.Layer):\n",
    "\n",
    "    def inference(\n",
    "        self,\n",
    "        input_feat, # learning_obj.input_feat\n",
    "        rho_coords, # learning_obj.rho_coords\n",
    "        theta_coords,\n",
    "        mask,\n",
    "        W_conv,\n",
    "        b_conv,\n",
    "        mu_rho,\n",
    "        sigma_rho,\n",
    "        mu_theta,\n",
    "        sigma_theta,\n",
    "        eps=1e-5,\n",
    "        mean_gauss_activation=True,\n",
    "    ):\n",
    "        n_samples = rho_coords.shape[1]\n",
    "        n_vertices = rho_coords.shape[2]\n",
    "        # print(\"n_samples\", rho_coords.shape)\n",
    "        # print(\"vertices\", n_vertices)\n",
    "\n",
    "        all_conv_feat = []\n",
    "        for k in range(self.n_rotations):\n",
    "            print(\"rotation\", k)\n",
    "            # print(\"\\trotation\", k+1)\n",
    "            # rho_coords_ = tf.reshape(rho_coords, [-1, 1])  # batch_size*n_vertices\n",
    "            rho_coords_ = tf.reshape(rho_coords, (-1,n_samples*n_vertices, 1))\n",
    "            # print(\"Brian rho_coords,rho_coords_\", rho_coords.shape, rho_coords_.shape)\n",
    "            # thetas_coords_ = tf.reshape(theta_coords, [-1, 1])  # batch_size*n_vertices\n",
    "            thetas_coords_ = tf.reshape(theta_coords, (-1,n_samples*n_vertices,1))\n",
    "            # print(\"thetas_coords,thetas_coords_\", theta_coords.shape, thetas_coords_.shape)\n",
    "            thetas_coords_ += k * 2 * np.pi / self.n_rotations\n",
    "            thetas_coords_ = tf.math.mod(thetas_coords_, 2 * np.pi)\n",
    "            rho_coords_ = tf.exp(\n",
    "                -tf.square(rho_coords_ - mu_rho) / (tf.square(sigma_rho) + eps)\n",
    "            )\n",
    "            thetas_coords_ = tf.exp(\n",
    "                -tf.square(thetas_coords_ - mu_theta) / (tf.square(sigma_theta) + eps)\n",
    "            )\n",
    "            \n",
    "            gauss_activations = tf.multiply(\n",
    "                rho_coords_, thetas_coords_\n",
    "            )  # batch_size*n_vertices, n_gauss\n",
    "            # print(\"gauss_activations shape 1\",gauss_activations.shape)\n",
    "            # print(\"gauss_activations1\", gauss_activations.shape, rho_coords_.shape, thetas_coords_.shape)\n",
    "            gauss_activations = tf.reshape(\n",
    "                # gauss_activations, [n_samples, n_vertices, -1]\n",
    "                gauss_activations,[-1,n_samples, n_vertices ,tf.shape(gauss_activations)[-1]]\n",
    "            )  # batch_size, n_vertices, n_gauss\n",
    "            # print(\"gauss_activations2\", gauss_activations.shape, n_samples, n_vertices,-1)\n",
    "            # print(\"gauss_activations shape 2\",gauss_activations.shape)\n",
    "            # print(\"gauss_activations,mask\", gauss_activations.shape, mask.shape)\n",
    "            gauss_activations = tf.multiply(gauss_activations, mask)\n",
    "            # print(\"this is actually working\")\n",
    "            \n",
    "            if (\n",
    "                mean_gauss_activation\n",
    "            ):  # computes mean weights for the different gaussians\n",
    "                gauss_activations /= (\n",
    "                    tf.reduce_sum(gauss_activations, 1, keepdims=True) + eps\n",
    "                )  # batch_size, n_vertices, n_gauss\n",
    "            # print(\"1\", gauss_activations.shape)\n",
    "            gauss_activations = tf.expand_dims(\n",
    "                # gauss_activations, 2\n",
    "                gauss_activations, 3\n",
    "            )  # batch_size, inputsize, n_vertices, 1, n_gauss,\n",
    "            # print(\"2\", gauss_activations.shape)\n",
    "            input_feat_ = tf.expand_dims(\n",
    "                # input_feat, 3\n",
    "                input_feat, 4\n",
    "            )  # batch_size, inputsize, n_vertices, n_feat, 1\n",
    "            # print(\"3\", input_feat_.shape)\n",
    "            gauss_desc = tf.multiply(\n",
    "                gauss_activations, input_feat_\n",
    "            )  # batch_size, n_vertices, n_feat, n_gauss,\n",
    "            # print(\"4\", gauss_desc.shape)\n",
    "            gauss_desc = tf.reduce_sum(gauss_desc, 2)  # batch_size, n_feat, n_gauss,\n",
    "            # print(\"5\", gauss_desc.shape)\n",
    "            gauss_desc = tf.reshape(\n",
    "                gauss_desc, [-1,n_samples, self.n_thetas * self.n_rhos]\n",
    "            )  # batch_size, 80\n",
    "            \n",
    "            # print(\"6\", gauss_desc.shape)\n",
    "            conv_feat = tf.matmul(gauss_desc, W_conv) + b_conv  # batch_size, 80\n",
    "            print(\"7\", conv_feat.shape)\n",
    "            # rho_coords2 = tf.identity(conv_feat)\n",
    "            all_conv_feat.append(conv_feat)\n",
    "\n",
    "        all_conv_feat = tf.stack(all_conv_feat)\n",
    "        conv_feat = tf.reduce_max(all_conv_feat, 0)\n",
    "        conv_feat = tf.nn.relu(conv_feat)\n",
    "        return conv_feat\n",
    "        # return 1,2\n",
    "        \n",
    "    def __init__(\n",
    "        self,\n",
    "        max_rho,\n",
    "        n_thetas=4,\n",
    "        n_rhos=3,\n",
    "        n_gamma=1.0,\n",
    "        learning_rate=1e-3,\n",
    "        n_rotations=1,\n",
    "        idx_gpu=\"/device:GPU:0\",\n",
    "        feat_mask=[1.0, 1.0, 1.0, 1.0, 1.0],\n",
    "        n_conv_layers=1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.max_rho=max_rho\n",
    "       \n",
    "        self.n_rhos=n_rhos\n",
    "        self.n_thetas=n_thetas\n",
    "        self.sigma_rho_init = (\n",
    "            max_rho / 8\n",
    "        ) \n",
    "        self.sigma_theta_init = 1.0\n",
    "        self.learning_rate=learning_rate\n",
    "        self.n_rotations=n_rotations\n",
    "        self.n_feat = int(sum(feat_mask))\n",
    "        self.n_labels = 3\n",
    "\n",
    "        initial_coords = self.compute_initial_coordinates()\n",
    "        self.initial_coords = initial_coords\n",
    "        mu_rho_initial = np.expand_dims(initial_coords[:, 0], 0).astype(\n",
    "                    \"float32\"\n",
    "                )\n",
    "        mu_theta_initial = np.expand_dims(initial_coords[:, 1], 0).astype(\n",
    "            \"float32\"\n",
    "        )\n",
    "        # print(mu_rho_initial[0:100])\n",
    "        self.mu_rho = []\n",
    "        self.mu_theta = []\n",
    "        self.sigma_rho = []\n",
    "        self.sigma_theta = []\n",
    "        for i in range(self.n_feat):\n",
    "            self.mu_rho.append(\n",
    "                tf.Variable(mu_rho_initial, name=\"mu_rho_{}\".format(i))\n",
    "            )  # 1, n_gauss\n",
    "            self.mu_theta.append(\n",
    "                tf.Variable(mu_theta_initial, name=\"mu_theta_{}\".format(i))\n",
    "            )  # 1, n_gauss\n",
    "            self.sigma_rho.append(\n",
    "                tf.Variable(\n",
    "                    np.ones_like(mu_rho_initial) * self.sigma_rho_init,\n",
    "                    name=\"sigma_rho_{}\".format(i),\n",
    "                )\n",
    "            )  # 1, n_gauss\n",
    "            self.sigma_theta.append(\n",
    "                tf.Variable(\n",
    "                    (np.ones_like(mu_theta_initial) * self.sigma_theta_init),\n",
    "                    name=\"sigma_theta_{}\".format(i),\n",
    "                )\n",
    "            )  # 1, n_gauss\n",
    "        \n",
    "        \n",
    "        self.b_conv = []\n",
    "        for i in range(self.n_feat):\n",
    "            self.b_conv.append(\n",
    "                tf.Variable(\n",
    "                    tf.zeros([self.n_thetas * self.n_rhos]),\n",
    "                    name=\"b_conv_{}\".format(i),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.W_conv = []\n",
    "        \n",
    "        for i in range(self.n_feat):\n",
    "            initializer=tf.keras.initializers.GlorotUniform(seed=(i+1)*100)\n",
    "            self.W_conv.append(\n",
    "                    tf.Variable(initializer((self.n_thetas * self.n_rhos, \n",
    "                                                self.n_thetas * self.n_rhos)),\n",
    "                                name=\"W_conv_{}\".format(i)\n",
    "                                )\n",
    "            )\n",
    "        self.mlp2= tf.keras.layers.Dense(\n",
    "                    self.n_thetas * self.n_rhos,\n",
    "                    activation=tf.nn.relu,\n",
    "                )\n",
    "        self.mlp3= tf.keras.layers.Dense(\n",
    "                    self.n_feat,\n",
    "                    activation=tf.nn.relu,\n",
    "                )\n",
    "        self.mlp4= tf.keras.layers.Dense(\n",
    "                    self.n_labels, activation='softmax'\n",
    "                )\n",
    "              \n",
    "        # self.b = self.add_weight(shape=(self.n_ligands*,), initializer=\"zeros\", trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # print(\"INPUTS!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\", len(inputs))\n",
    "        input_feat=inputs[0]\n",
    "        print(\"\\tinput_feat\", input_feat.shape)\n",
    "        rho_coords=inputs[1]\n",
    "        # print(\"\\trho_coords\", rho_coords.shape)\n",
    "        theta_coords=inputs[2]\n",
    "        # print(\"\\ttheta_coords\", theta_coords.shape)\n",
    "        mask=inputs[3]\n",
    "        # print(\"\\tmask\", mask.shape)\n",
    "        # self.keep_prob=inputs[4]\n",
    "        # labels=inputs[5]\n",
    "    \n",
    "        self.global_desc_1 = []\n",
    "        self.rho_coords2 = []\n",
    "        for i in range(self.n_feat):\n",
    "        # for i in range(1):\n",
    "            print('feature',['si', 'ddc', 'hbond', 'charge', 'hphob'][i])\n",
    "            # my_input_feat = tf.keras.ops.expand_dims(input_feat[:, :, i], 2)\n",
    "            my_input_feat = tf.keras.ops.expand_dims(input_feat[:, :, :, i], 3)\n",
    "            # print(\"WHAT\", my_input_feat.shape)\n",
    "            conv_feat = self.inference(\n",
    "                            my_input_feat,\n",
    "                            rho_coords,\n",
    "                            theta_coords,\n",
    "                            mask,\n",
    "                            self.W_conv[i],\n",
    "                            self.b_conv[i],\n",
    "                            self.mu_rho[i],\n",
    "                            self.sigma_rho[i],\n",
    "                            self.mu_theta[i],\n",
    "                            self.sigma_theta[i],\n",
    "                        )\n",
    "            self.global_desc_1.append(conv_feat)\n",
    "\n",
    "        self.global_desc_1 = tf.stack(self.global_desc_1, axis=2)  \n",
    "        # print(\"8\", self.global_desc_1.shape)\n",
    "\n",
    "        self.global_desc_1 = tf.reshape(\n",
    "                    self.global_desc_1, [-1,500, self.n_thetas * self.n_rhos * self.n_feat]\n",
    "                )\n",
    "        # print(\"9\", self.global_desc_1.shape)\n",
    "\n",
    "        self.global_desc_1=self.mlp2(self.global_desc_1)\n",
    "        # print(\"10\", self.global_desc_1.shape)\n",
    "\n",
    "        self.global_desc_1=self.mlp3(self.global_desc_1)\n",
    "        # print(\"11\", self.global_desc_1.shape)\n",
    "       \n",
    "        self.logits=self.mlp4(self.global_desc_1)\n",
    "        print(\"shape logits\", self.logits.shape)\n",
    "       \n",
    "        # return self.global_desc_2, self.logits, self.rho_coords2\n",
    "        return self.logits\n",
    "\n",
    "\n",
    "    def compute_initial_coordinates(self):\n",
    "        range_rho = [0.0, self.max_rho]\n",
    "        range_theta = [0, 2 * np.pi]\n",
    "\n",
    "        grid_rho = np.linspace(range_rho[0], range_rho[1], num=self.n_rhos + 1)\n",
    "        grid_rho = grid_rho[1:]\n",
    "        grid_theta = np.linspace(range_theta[0], range_theta[1], num=self.n_thetas + 1)\n",
    "        grid_theta = grid_theta[:-1]\n",
    "\n",
    "        grid_rho_, grid_theta_ = np.meshgrid(grid_rho, grid_theta, sparse=False)\n",
    "        grid_rho_ = (\n",
    "            grid_rho_.T\n",
    "        )  # the traspose here is needed to have the same behaviour as Matlab code\n",
    "        grid_theta_ = (\n",
    "            grid_theta_.T\n",
    "        )  # the traspose here is needed to have the same behaviour as Matlab code\n",
    "        grid_rho_ = grid_rho_.flatten()\n",
    "        grid_theta_ = grid_theta_.flatten()\n",
    "\n",
    "        coords = np.concatenate((grid_rho_[None, :], grid_theta_[None, :]), axis=0)\n",
    "        coords = coords.T  # every row contains the coordinates of a grid intersection\n",
    "        # print(coords.shape)\n",
    "        return coords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "\n",
    "class MyModel(keras.Model):\n",
    "  def __init__(self):\n",
    "    super(MyModel, self).__init__()\n",
    "    self.conv1 = MaSIF_site2(\n",
    "        params[\"max_distance\"],\n",
    "        n_thetas=4,\n",
    "        n_rhos=3,\n",
    "        n_rotations=1,\n",
    "        idx_gpu=\"/gpu:1\",\n",
    "        feat_mask=[1.0]*5,\n",
    "        n_conv_layers=params[\"n_conv_layers\"],\n",
    "    )\n",
    "\n",
    "  def call(self, inputs):\n",
    "    logits = self.conv1(inputs)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glorot_uniform(shape, seed=None):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    limit = np.sqrt(6 / (shape[0] + shape[1]))\n",
    "    return np.random.uniform(-limit, limit, size=shape).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaSIF_ligand2(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_rho,\n",
    "        n_ligands,\n",
    "        n_thetas=16,\n",
    "        n_rhos=5,\n",
    "        n_gamma=1.0,\n",
    "        learning_rate=1e-4,\n",
    "        n_rotations=16,\n",
    "        idx_gpu=\"/gpu:0\",\n",
    "        feat_mask=[1.0, 1.0, 1.0, 1.0],\n",
    "        costfun=\"dprime\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.max_rho=max_rho\n",
    "        self.n_ligands=n_ligands\n",
    "        self.n_rhos=n_rhos\n",
    "        self.n_thetas=n_thetas\n",
    "        self.sigma_rho_init = (\n",
    "            max_rho / 8\n",
    "        ) \n",
    "        self.sigma_theta_init = 1.0\n",
    "        self.learning_rate=learning_rate\n",
    "        self.n_rotations=n_rotations\n",
    "        self.n_feat = int(sum(feat_mask))\n",
    "\n",
    "        initial_coords = self.compute_initial_coordinates()\n",
    "        mu_rho_initial = np.expand_dims(initial_coords[:, 0], 0).astype(\n",
    "                    \"float32\"\n",
    "                )\n",
    "        mu_theta_initial = np.expand_dims(initial_coords[:, 1], 0).astype(\n",
    "            \"float32\"\n",
    "        )\n",
    "        print(mu_rho_initial[0:100])\n",
    "        self.mu_rho = []\n",
    "        self.mu_theta = []\n",
    "        self.sigma_rho = []\n",
    "        self.sigma_theta = []\n",
    "        for i in range(self.n_feat):\n",
    "            self.mu_rho.append(\n",
    "                tf.Variable(mu_rho_initial, name=\"mu_rho_{}\".format(i))\n",
    "            )  # 1, n_gauss\n",
    "            self.mu_theta.append(\n",
    "                tf.Variable(mu_theta_initial, name=\"mu_theta_{}\".format(i))\n",
    "            )  # 1, n_gauss\n",
    "            self.sigma_rho.append(\n",
    "                tf.Variable(\n",
    "                    np.ones_like(mu_rho_initial) * self.sigma_rho_init,\n",
    "                    name=\"sigma_rho_{}\".format(i),\n",
    "                )\n",
    "            )  # 1, n_gauss\n",
    "            self.sigma_theta.append(\n",
    "                tf.Variable(\n",
    "                    (np.ones_like(mu_theta_initial) * self.sigma_theta_init),\n",
    "                    name=\"sigma_theta_{}\".format(i),\n",
    "                )\n",
    "            )  # 1, n_gauss\n",
    "        \n",
    "        \n",
    "        self.b_conv = []\n",
    "        for i in range(self.n_feat):\n",
    "            self.b_conv.append(\n",
    "                tf.Variable(\n",
    "                    tf.zeros([self.n_thetas * self.n_rhos]),\n",
    "                    name=\"b_conv_{}\".format(i),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.W_conv = []\n",
    "        initializer=tf.keras.initializers.glorot_normal(seed=0)\n",
    "        for i in range(self.n_feat):\n",
    "            self.W_conv.append(\n",
    "                    tf.Variable(glorot_uniform((80, 80), seed=(i+1)*100),\n",
    "                                name=\"W_conv_{}\".format(i)\n",
    "                                )\n",
    "            )\n",
    "        # self.b = self.add_weight(shape=(self.n_ligands*,), initializer=\"zeros\", trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_feat=inputs[0]\n",
    "        rho_coords=inputs[1]\n",
    "        theta_coords=inputs[2]\n",
    "        mask=inputs[3]\n",
    "        labels=inputs[4]\n",
    "        self.keep_prob=inputs[5]\n",
    "    \n",
    "        self.global_desc_1 = []\n",
    "        self.rho_coords2 = []\n",
    "        for i in range(self.n_feat):\n",
    "            print('feature',['si', 'ddc', 'hbond', 'charge', 'hphob'][i])\n",
    "            my_input_feat = tf.keras.ops.expand_dims(input_feat[:, :, i], 2)\n",
    "            conv_feat, rho_coords2, = self.inference(\n",
    "                            my_input_feat,\n",
    "                            rho_coords,\n",
    "                            theta_coords,\n",
    "                            mask,\n",
    "                            self.W_conv[i],\n",
    "                            self.b_conv[i],\n",
    "                            self.mu_rho[i],\n",
    "                            self.sigma_rho[i],\n",
    "                            self.mu_theta[i],\n",
    "                            self.sigma_theta[i],\n",
    "                            \n",
    "                        )\n",
    "            self.global_desc_1.append(conv_feat)\n",
    "            self.rho_coords2.append(rho_coords2)\n",
    "\n",
    "        self.global_desc_1 = tf.stack(self.global_desc_1, axis=1)\n",
    "        self.global_desc_1 = tf.reshape(\n",
    "                    self.global_desc_1, [-1, self.n_thetas * self.n_rhos * self.n_feat]\n",
    "                )\n",
    "        \n",
    "        self.global_desc_1 = tf.keras.layers.Dense(\n",
    "                    self.n_thetas * self.n_rhos,\n",
    "                    activation=tf.nn.relu,\n",
    "                )(self.global_desc_1)\n",
    "\n",
    "        self.global_desc_1 = tf.matmul(\n",
    "                    tf.transpose(self.global_desc_1), self.global_desc_1\n",
    "                ) / tf.cast(tf.shape(self.global_desc_1)[0], tf.float32)\n",
    "        self.global_desc_2 = self.global_desc_1\n",
    "        # self.global_desc_2 = self.global_desc_1\n",
    "        # self.global_desc_2 = self.global_desc_1\n",
    "        self.global_desc_1 = tf.reshape(self.global_desc_1, [1, -1])\n",
    "        self.global_desc_1 = tf.nn.dropout(self.global_desc_1, self.keep_prob)\n",
    "        self.global_desc_1 = tf.keras.layers.Dense(\n",
    "            64, activation=tf.nn.relu\n",
    "        )(self.global_desc_1)\n",
    "        \n",
    "        self.logits = tf.keras.layers.Dense(\n",
    "                    self.n_ligands, activation=tf.identity\n",
    "        )(self.global_desc_1)\n",
    "        \n",
    "        return self.global_desc_2, self.logits, self.rho_coords2\n",
    "\n",
    "    def inference(\n",
    "        self,\n",
    "        input_feat, # learning_obj.input_feat\n",
    "        rho_coords, # learning_obj.rho_coords\n",
    "        theta_coords,\n",
    "        mask,\n",
    "        W_conv,\n",
    "        b_conv,\n",
    "        mu_rho,\n",
    "        sigma_rho,\n",
    "        mu_theta,\n",
    "        sigma_theta,\n",
    "        eps=1e-5,\n",
    "        mean_gauss_activation=True,\n",
    "    ):\n",
    "        n_samples = tf.shape(rho_coords)[0]\n",
    "        n_vertices = tf.shape(rho_coords)[1]\n",
    "      \n",
    "\n",
    "        all_conv_feat = []\n",
    "        for k in range(self.n_rotations):\n",
    "            print(\"\\trotation\", k+1)\n",
    "            rho_coords_ = tf.reshape(rho_coords, [-1, 1])  # batch_size*n_vertices\n",
    "            thetas_coords_ = tf.reshape(theta_coords, [-1, 1])  # batch_size*n_vertices\n",
    "\n",
    "            thetas_coords_ += k * 2 * np.pi / self.n_rotations\n",
    "            thetas_coords_ = tf.math.mod(thetas_coords_, 2 * np.pi)\n",
    "            rho_coords_ = tf.exp(\n",
    "                -tf.square(rho_coords_ - mu_rho) / (tf.square(sigma_rho) + eps)\n",
    "            )\n",
    "            thetas_coords_ = tf.exp(\n",
    "                -tf.square(thetas_coords_ - mu_theta) / (tf.square(sigma_theta) + eps)\n",
    "            )\n",
    "            \n",
    "            gauss_activations = tf.multiply(\n",
    "                rho_coords_, thetas_coords_\n",
    "            )  # batch_size*n_vertices, n_gauss\n",
    "            # print(\"gauss_activations shape 1\",gauss_activations.shape)\n",
    "            \n",
    "            gauss_activations = tf.reshape(\n",
    "                gauss_activations, [n_samples, n_vertices, -1]\n",
    "            )  # batch_size, n_vertices, n_gauss\n",
    "            # print(\"gauss_activations shape 2\",gauss_activations.shape)\n",
    "            gauss_activations = tf.multiply(gauss_activations, mask)\n",
    "            \n",
    "            if (\n",
    "                mean_gauss_activation\n",
    "            ):  # computes mean weights for the different gaussians\n",
    "                gauss_activations /= (\n",
    "                    tf.reduce_sum(gauss_activations, 1, keepdims=True) + eps\n",
    "                )  # batch_size, n_vertices, n_gauss\n",
    "            \n",
    "            gauss_activations = tf.expand_dims(\n",
    "                gauss_activations, 2\n",
    "            )  # batch_size, n_vertices, 1, n_gauss,\n",
    "            input_feat_ = tf.expand_dims(\n",
    "                input_feat, 3\n",
    "            )  # batch_size, n_vertices, n_feat, 1\n",
    "\n",
    "            gauss_desc = tf.multiply(\n",
    "                gauss_activations, input_feat_\n",
    "            )  # batch_size, n_vertices, n_feat, n_gauss,\n",
    "            \n",
    "            gauss_desc = tf.reduce_sum(gauss_desc, 1)  # batch_size, n_feat, n_gauss,\n",
    "            gauss_desc = tf.reshape(\n",
    "                gauss_desc, [n_samples, self.n_thetas * self.n_rhos]\n",
    "            )  # batch_size, 80\n",
    "            \n",
    "            \n",
    "            conv_feat = tf.matmul(gauss_desc, W_conv) + b_conv  # batch_size, 80\n",
    "            rho_coords2 = tf.identity(conv_feat)\n",
    "            all_conv_feat.append(conv_feat)\n",
    "\n",
    "        all_conv_feat = tf.stack(all_conv_feat)\n",
    "        conv_feat = tf.reduce_max(all_conv_feat, 0)\n",
    "        conv_feat = tf.nn.relu(conv_feat)\n",
    "        return conv_feat, rho_coords2\n",
    "\n",
    "\n",
    "    def compute_initial_coordinates(self):\n",
    "        range_rho = [0.0, self.max_rho]\n",
    "        range_theta = [0, 2 * np.pi]\n",
    "\n",
    "        grid_rho = np.linspace(range_rho[0], range_rho[1], num=self.n_rhos + 1)\n",
    "        grid_rho = grid_rho[1:]\n",
    "        grid_theta = np.linspace(range_theta[0], range_theta[1], num=self.n_thetas + 1)\n",
    "        grid_theta = grid_theta[:-1]\n",
    "\n",
    "        grid_rho_, grid_theta_ = np.meshgrid(grid_rho, grid_theta, sparse=False)\n",
    "        grid_rho_ = (\n",
    "            grid_rho_.T\n",
    "        )  # the traspose here is needed to have the same behaviour as Matlab code\n",
    "        grid_theta_ = (\n",
    "            grid_theta_.T\n",
    "        )  # the traspose here is needed to have the same behaviour as Matlab code\n",
    "        grid_rho_ = grid_rho_.flatten()\n",
    "        grid_theta_ = grid_theta_.flatten()\n",
    "\n",
    "        coords = np.concatenate((grid_rho_[None, :], grid_theta_[None, :]), axis=0)\n",
    "        coords = coords.T  # every row contains the coordinates of a grid intersection\n",
    "        print(coords.shape)\n",
    "        return coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.set_random_seed(\n",
    "    0\n",
    ")\n",
    "\n",
    "global_desc_2, logits, rho_coords2 = MaSIF_ligand2(\n",
    "    max_rho=params[\"max_distance\"],\n",
    "    n_ligands=params[\"n_classes\"],\n",
    "    feat_mask=params[\"feat_mask\"],\n",
    "    n_rotations=2\n",
    ")([input_feat,rho_coords,theta_coords,mask,pocket_labels,tf.constant(1.0)])\n",
    "\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global_desc_1= tf.matmul(\n",
    "#                     tf.transpose(global_desc_2), global_desc_2\n",
    "#                 ) / tf.cast(tf.shape(global_desc_2)[0], tf.float32)\n",
    "global_desc_1 = tf.reshape(global_desc_2, [1, -1])\n",
    "print(global_desc_1.numpy().sum())\n",
    "global_desc_1 = tf.nn.dropout(global_desc_1, 0)\n",
    "print(global_desc_1.numpy().sum())\n",
    "global_desc_1 = tf.keras.layers.Dense(\n",
    "                    # self.global_desc_1, \n",
    "                    64, activation=tf.nn.relu\n",
    "                )(global_desc_1)\n",
    "print(global_desc_1.numpy().sum())\n",
    "logits = tf.keras.layers.Dense(\n",
    "                    # self.global_desc_1, \n",
    "                    10, activation=tf.identity\n",
    "                )(global_desc_1)\n",
    "logits.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.set_random_seed(1)\n",
    "W_conv = []\n",
    "# initializer=tf.keras.initializers.glorot_normal(seed=10)\n",
    "for i in range(5):\n",
    "    \n",
    "    W_conv.append(\n",
    "            glorot_uniform((80, 80), seed=(i+1)*100)\n",
    "                        \n",
    "    )\n",
    "[i.sum() for i in W_conv]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.set_random_seed(1)\n",
    "W_conv = []\n",
    "initializer=tf.compat.v1.keras.initializers.glorot_normal(seed=10)\n",
    "# initializer=tf.keras.initializers.glorot_normal(seed=10)\n",
    "for i in range(5):\n",
    "    W_conv.append(\n",
    "           initializer(shape=(80, 80))\n",
    "    )\n",
    "[i.numpy().sum() for i in W_conv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params[\"n_classes\"]=10\n",
    "\n",
    "layer = MaSIF_ligand2(\n",
    "    max_rho=params[\"max_distance\"],\n",
    "    n_ligands=params[\"n_classes\"],\n",
    "    feat_mask=params[\"feat_mask\"]\n",
    ")\n",
    "# _ = layer(ops.zeros((1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from masif_modules.read_ligand_tfrecords import _parse_function\n",
    "\n",
    "params = masif_opts[\"ligand\"]\n",
    "\n",
    "# Load dataset\n",
    "training_data = tf.data.TFRecordDataset(\n",
    "    os.path.join(params[\"tfrecords_dir\"], \"training_data.tfrecord\")\n",
    ")\n",
    "validation_data = tf.data.TFRecordDataset(\n",
    "    os.path.join(params[\"tfrecords_dir\"], \"validation_data.tfrecord\")\n",
    ")\n",
    "testing_data = tf.data.TFRecordDataset(\n",
    "    os.path.join(params[\"tfrecords_dir\"], \"testing_data.tfrecord\")\n",
    ")\n",
    "training_data = training_data.map(_parse_function)\n",
    "validation_data = validation_data.map(_parse_function)\n",
    "testing_data = testing_data.map(_parse_function)\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "training_iterator = iter(training_data)\n",
    "# training_iterator=training_data.make_one_shot_iterator()\n",
    "# data_element = training_iterator.get_next()\n",
    "# data_element[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import pymesh\n",
    "import numpy as np\n",
    "import sys\n",
    "test = [x.rstrip().split('.')[0] for x in open(\"feat.txt\").readlines()]\n",
    "# training_list = [x.rstrip() for x in open(params['training_list']).readlines()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict = {}\n",
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"data_preparation/01-benchmark_surfaces/\"+f+\"_protein.ply\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import pymesh\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "test = [x.rstrip().split('.')[0] for x in open(\"names.txt\").readlines()]\n",
    "my_dict = {}\n",
    "\n",
    "for i, f in enumerate(test[3300:3600]):\n",
    "    if i % 10 == 0:\n",
    "        print(i,f)\n",
    "        sys.stdout.flush()\n",
    "    m=pymesh.load_mesh(\"data_preparation/01-benchmark_surfaces/\"+f+\"_protein.ply\")\n",
    "    n=np.load(\"data_preparation/01-benchmark_surfaces/\"+f+\"_names.npy\",\n",
    "        allow_pickle=True).item()['names1']\n",
    "\n",
    "    one = n[m.get_attribute(\"vertex_iface\")==1]\n",
    "    if len(one) > 0:\n",
    "        if len(one.shape) > 1:\n",
    "            one = np.concatenate(one)\n",
    "        one = ['_'.join(np.array(i.split('_'))[1:4]) for i in one]\n",
    "        if '384_x_TYR' in one:\n",
    "            print(f)\n",
    "        for name in one:\n",
    "            if name in my_dict:\n",
    "                my_dict[name]['1'] +=1\n",
    "            else:\n",
    "                my_dict[name] = {}\n",
    "                my_dict[name]['1'] = 1\n",
    "            if '2' not in my_dict[name]:\n",
    "                my_dict[name]['2'] = 0\n",
    "    two = n[m.get_attribute(\"vertex_iface\")==2]\n",
    "    if len(two) > 0:\n",
    "        two = ['_'.join(np.array(i.split('_'))[1:4]) for i in np.concatenate(two)]\n",
    "        for name in two:\n",
    "            if name in my_dict:\n",
    "                my_dict[name]['2'] +=1\n",
    "            else:\n",
    "                my_dict[name] = {}\n",
    "                my_dict[name]['2'] = 1\n",
    "        \n",
    "res = OrderedDict(sorted(my_dict.items(),\n",
    "       key = lambda x: x[1]['2'], reverse=True))\n",
    "np.save(\"two.npy\", res)\n",
    "res = OrderedDict(sorted(my_dict.items(),\n",
    "       key = lambda x: x[1]['1'], reverse=True))\n",
    "np.save(\"one.npy\", res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "res = OrderedDict(sorted(my_dict.items(),\n",
    "       key = lambda x: x[1]['2'], reverse=True))\n",
    "res\n",
    "np.save(\"two.npy\", res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=np.load(\"one.npy\", allow_pickle=True).item()\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(sorted(my_dict.items(), key = lambda x: x['1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymesh\n",
    "mesh=pymesh.load_mesh(\"data_preparation/01-benchmark_surfaces/Sim_1_traj_10_frame_1282_protein.ply\")\n",
    "(mesh.get_attribute('vertex_iface')==1).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precom_dir = params[\"masif_precomputation_dir\"]\n",
    "ligand_coord_dir = params[\"ligand_coords_dir\"]\n",
    "tfrecords_dir = params[\"tfrecords_dir\"]\n",
    "params[\"n_classes\"]=10\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "for num_train_sample in range(1):\n",
    "# for num_train_sample in range(int(num_training_samples / 10)):\n",
    "    try:\n",
    "        # data_element = training_iterator.get_next()\n",
    "        data_element = list(training_iterator)[0]\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    labels = data_element[4]\n",
    "    print(data_element[5])\n",
    "    n_ligands = labels.shape[1]\n",
    "    # Choose a random ligand from the structure\n",
    "    all_ligand_types = np.load(\n",
    "        os.path.join(\n",
    "            ligand_coord_dir, \"{}_ligtypes.npy\".format(data_element[5].numpy().decode(\"utf-8\"))\n",
    "        )\n",
    "    ).astype(str)\n",
    "    # random_ligand = np.random.choice(n_ligands, 1) # TODO: don't choose cofactor?\n",
    "    \n",
    "    random_ligand = np.random.choice(n_ligands-1, 1)+1 # TODO: don't choose cofactor?\n",
    "    pocket_points = np.where(labels.numpy()[:, random_ligand] != 0.0)[0]\n",
    "    label = np.max(labels.numpy()[:, random_ligand]) - 1\n",
    "    pocket_labels = np.zeros(params[\"n_classes\"], dtype=np.float32)\n",
    "    pocket_labels[label] = 1.0\n",
    "    npoints = pocket_points.shape[0]\n",
    "    if npoints < 32:\n",
    "        print('less than 32:', npoints, data_element[5], all_ligand_types[random_ligand])\n",
    "        continue\n",
    "    else:\n",
    "        print(npoints, data_element[5], all_ligand_types[random_ligand])\n",
    "\n",
    "    input_feat=data_element[0].numpy()[pocket_points[:32], :, :]\n",
    "    rho_coords=np.expand_dims(data_element[1], -1)[pocket_points[:32], :, :]\n",
    "    theta_coords=np.expand_dims(data_element[2], -1)[pocket_points[:32], :, :]\n",
    "    mask=data_element[3].numpy()[pocket_points[:32], :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feed_dict = {\n",
    "#         'Input_feat': input_feat,\n",
    "#         'rho_coords':rho_coords,\n",
    "#         'theta_coords': theta_coords,\n",
    "#         'mask': mask,\n",
    "#         'labels': pocket_labels,\n",
    "#         'keep_prob': 1.0\n",
    "#     }\n",
    "# feed_dict = [input_feat,rho_coords,theta_coords,mask,pocket_labels,1.0]\n",
    "\n",
    "global_desc_1, rho_coords2 = MaSIF_ligand2(\n",
    "    max_rho=params[\"max_distance\"],\n",
    "    n_ligands=params[\"n_classes\"],\n",
    "    feat_mask=params[\"feat_mask\"],\n",
    "    n_rotations=2\n",
    ")([input_feat,rho_coords,theta_coords,mask,pocket_labels,tf.constant(1.0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "batch_size=200\n",
    "params[\"n_conv_layers\"]=1\n",
    "\n",
    "ppi_pair_id=\"Sim_1_traj_42_frame_1307\"\n",
    "mydir=\"data_preparation/04a-precomputation_12A/precomputation/\" + ppi_pair_id + \"/\"\n",
    "pdbid = ppi_pair_id.split(\".\")[0]\n",
    "pid=pdbid\n",
    "rho_wrt_center = np.load(mydir + pid + \"_rho_wrt_center.npy\")\n",
    "theta_wrt_center = np.load(mydir + pid + \"_theta_wrt_center.npy\")\n",
    "input_feat = np.load(mydir + pid + \"_input_feat.npy\")\n",
    "\n",
    "iface_labels = np.load(mydir + pid + \"_iface_labels.npy\")\n",
    "\n",
    "mask = np.load(mydir + pid + \"_mask.npy\")\n",
    "mask = np.expand_dims(mask, 2)\n",
    "indices = np.load(mydir + pid + \"_list_indices.npy\", allow_pickle=True).item()['protein']\n",
    "\n",
    "# indices is (n_verts x <30), it should be\n",
    "# indices = pad_indices(indices, mask.shape[1])\n",
    "tmp = np.zeros((len(iface_labels), 3))\n",
    "for i in range(len(iface_labels)):\n",
    "    if iface_labels[i] == 1:\n",
    "        tmp[i, 1] = 1\n",
    "    elif iface_labels[i] == 2:\n",
    "        tmp[i, 2] = 1\n",
    "    else:\n",
    "        tmp[i, 0] = 1\n",
    "iface_labels_dc = tmp\n",
    "super_pos = np.where(iface_labels == 2)[0]\n",
    "pos_labels = np.where(iface_labels == 1)[0]\n",
    "neg_labels = np.where(iface_labels == 0)[0]\n",
    "print(\"PNS\",len(pos_labels),len(neg_labels), len(super_pos))\n",
    "\n",
    "\n",
    "n = min(len(pos_labels), len(neg_labels))\n",
    "n = min(n, batch_size // 2)\n",
    "subset = np.concatenate([neg_labels[:(n*5)], pos_labels[:n], super_pos])\n",
    "print(len(subset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num_train_sample in range(1):\n",
    "# for num_train_sample in range(int(num_training_samples / 10)):\n",
    "    try:\n",
    "        data_element = sess.run(training_next_element)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    labels = data_element[4]\n",
    "    n_ligands = labels.shape[1]\n",
    "    # Choose a random ligand from the structure\n",
    "    all_ligand_types = np.load(\n",
    "        os.path.join(\n",
    "            ligand_coord_dir, \"{}_ligtypes.npy\".format(data_element[5].decode(\"utf-8\"))\n",
    "        )\n",
    "    ).astype(str)\n",
    "    random_ligand = np.random.choice(n_ligands, 1) # TODO: don't choose cofactor?\n",
    "    pocket_points = np.where(labels[:, random_ligand] != 0.0)[0]\n",
    "    label = np.max(labels[:, random_ligand]) - 1\n",
    "    pocket_labels = np.zeros(params[\"n_classes\"], dtype=np.float32)\n",
    "    pocket_labels[label] = 1.0\n",
    "    npoints = pocket_points.shape[0]\n",
    "    if npoints < 32:\n",
    "        print('less than 32:', npoints, data_element[5], all_ligand_types[random_ligand])\n",
    "        continue\n",
    "    else:\n",
    "        print(npoints, data_element[5], all_ligand_types[random_ligand])\n",
    "\n",
    "    input_feat=data_element[0][pocket_points[:32], :, :]\n",
    "    rho_coords=np.expand_dims(data_element[1], -1)[pocket_points[:32], :, :]\n",
    "    theta_coords=np.expand_dims(data_element[2], -1)[pocket_points[:32], :, :]\n",
    "    mask=data_element[3][pocket_points[:32], :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from masif_modules.read_ligand_tfrecords import _parse_function\n",
    "# Load dataset\n",
    "training_data = tf.data.TFRecordDataset(\n",
    "    os.path.join(params[\"tfrecords_dir\"], \"training_data.tfrecord\")\n",
    ")\n",
    "training_data = training_data.map(_parse_function)\n",
    "# training_iterator = training_data.make_one_shot_iterator()\n",
    "# training_next_element = training_iterator.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batched_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from input_output.save_ply import save_ply\n",
    "save_ply(\"mesh_test\"+\".ply\", mesh.vertices,\n",
    "         mesh.faces, normals=n, \n",
    "         charges=mesh.get_attribute(\"vertex_charge\"),\n",
    "         normalize_charges=False, \n",
    "         hbond=mesh.get_attribute(\"vertex_hbond\"), \n",
    "         hphob=mesh.get_attribute(\"vertex_hphob\"),\n",
    "         iface=mesh.get_attribute(\"vertex_iface\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from masif_modules.read_data_from_surface import read_data_from_surface, compute_shape_complementarity\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "params = masif_opts['site']\n",
    "masif_opts[\"ply_file_template\"] = masif_opts[\"ply_chain_dir\"] + \"/{}_{}.ply\"\n",
    "\n",
    "print(masif_opts['ply_file_template'])\n",
    "# plane 7037\n",
    "ppi_pair_list=['1A0H_D']\n",
    "for ppi_pair_id in ppi_pair_list:\n",
    "    print(ppi_pair_id)\n",
    "    all_list_desc = []\n",
    "    all_list_coords = []\n",
    "    all_list_shape_idx = []\n",
    "    all_list_names = []\n",
    "    idx_positives = []\n",
    "    # fields=ppi_pair_id.split('.')\n",
    "    fields=ppi_pair_id.split('_')\n",
    "\n",
    "    my_precomp_dir = params['masif_precomputation_dir']\n",
    "    if not os.path.exists(my_precomp_dir):\n",
    "        os.makedirs(my_precomp_dir)\n",
    "\n",
    "    ply_file = {}\n",
    "    ply_file['p1'] = masif_opts['ply_file_template'].format(fields[0], fields[1])\n",
    "    if len (fields) == 2 or fields[2] == '':\n",
    "        pids = ['p1']\n",
    "    \n",
    "    rho = {}\n",
    "    neigh_indices = {}\n",
    "    mask = {}\n",
    "    input_feat = {}\n",
    "    theta = {}\n",
    "    iface_labels = {}\n",
    "    verts = {}\n",
    "\n",
    "    pid=pids[0]\n",
    "    print(ply_file[pid])\n",
    "    (input_feat[pid], \n",
    "     rho[pid], \n",
    "     theta[pid], \n",
    "     mask[pid], \n",
    "     neigh_indices[pid], \n",
    "     iface_labels[pid], \n",
    "     verts[pid]) = read_data_from_surface(ply_file[pid], \n",
    "                                          params, \n",
    "                                          fields[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb='Sim_1_traj_10_frame_1236'\n",
    "regular_mesh=pymesh.load_mesh('data_preparation/01-benchmark_surfaces/%s_protein.ply' % pdb)\n",
    "regular_mesh.get_attribute(\"vertex_iface\").sum()\n",
    "\n",
    "# full_regular_mesh=pymesh.load_mesh('data_preparation/01-benchmark_surfaces/%s_protein_complex.ply' % pdb)\n",
    "# full_regular_mesh.get_attribute(\"vertex_iface\").sum()\n",
    "\n",
    "regular_names = np.load('data_preparation/01-benchmark_surfaces/%s_names.npy' % pdb, allow_pickle=True).item()\n",
    "\n",
    "full_regular_names = np.load('data_preparation/01-benchmark_surfaces/%s_fullnames.npy' % pdb, allow_pickle=True).item()\n",
    "\n",
    "regular_names=np.concatenate(regular_names['names1'])\n",
    "full_regular_names=full_regular_names['names1']\n",
    "print(len(full_regular_names), len(regular_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network ligand application specific parameters.\n",
    "import os\n",
    "import sys\n",
    "from multiprocessing import Pool\n",
    "import subprocess as sp\n",
    "import itertools\n",
    "\n",
    "sys.path.insert(0, \"/data/pompei/bw973/Oxygenases/masif/source\")\n",
    "from default_config.masif_opts import masif_opts\n",
    "masif_opts[\"ply_file_template\"] = masif_opts[\"ply_chain_dir\"] + \"/{}_protein.ply\"\n",
    "masif_opts[\"ligand\"] = {}\n",
    "masif_opts[\"ligand\"][\"assembly_dir\"] = \"data_preparation/00b-pdbs_assembly/\"\n",
    "masif_opts[\"ligand\"][\"assembly_dir2\"] = \"data_preparation/00c-pdbs_assembly/\"\n",
    "masif_opts[\"ligand\"][\"ligand_coords_dir\"] = \"data_preparation/00c-ligand_coords/\"\n",
    "masif_opts[\"ligand\"][\n",
    "    \"masif_precomputation_dir\"\n",
    "] = \"data_preparation/04a-precomputation_12A/precomputation/\"\n",
    "masif_opts[\"ligand\"][\"max_shape_size\"] = 50\n",
    "masif_opts[\"ligand\"][\"feat_mask\"] = [1.0] * 5\n",
    "masif_opts[\"ligand\"][\"train_fract\"] = 0.9 * 0.8\n",
    "masif_opts[\"ligand\"][\"val_fract\"] = 0.1 * 0.8\n",
    "masif_opts[\"ligand\"][\"test_fract\"] = 0.2\n",
    "masif_opts[\"ligand\"][\"tfrecords_dir\"] = \"data_preparation/tfrecords/\"\n",
    "masif_opts[\"ligand\"][\"max_distance\"] = 5.0\n",
    "masif_opts[\"ligand\"][\"n_classes\"] = 7\n",
    "masif_opts[\"ligand\"][\"feat_mask\"] = [1.0, 1.0, 1.0, 1.0, 1.0]\n",
    "masif_opts[\"ligand\"][\"costfun\"] = \"dprime\"\n",
    "masif_opts[\"ligand\"][\"model_dir\"] = \"nn_models/all_feat/\"\n",
    "masif_opts[\"ligand\"][\"test_set_out_dir\"] = \"test_set_predictions/\"\n",
    "\n",
    "def convert_to_string(binary):\n",
    "    return binary.decode('utf-8')\n",
    "\n",
    "def _run_command(cmd_info):\n",
    "    \"\"\"Helper function for submitting commands parallelized.\"\"\"\n",
    "    cmd, supress = cmd_info\n",
    "    p = sp.Popen(cmd, shell=True, stdout=sp.PIPE, stderr=sp.PIPE)\n",
    "    output, err = p.communicate()\n",
    "    if convert_to_string(err) != '' and not supress:\n",
    "        print(\"\\nERROR: \" + convert_to_string(err))\n",
    "        raise\n",
    "    output = convert_to_string(output)\n",
    "    p.terminate()\n",
    "    return output\n",
    "\n",
    "\n",
    "def run_commands(cmds, supress=False, n_procs=1):\n",
    "    \"\"\"Wrapper for submitting commands to shell\"\"\"\n",
    "    if type(cmds) is str:\n",
    "        cmds = [cmds]\n",
    "    if n_procs == 1:\n",
    "        outputs = []\n",
    "        for cmd in cmds:\n",
    "            outputs.append(_run_command((cmd, supress)))\n",
    "    else:\n",
    "        cmd_info = list(zip(cmds, itertools.repeat(supress)))\n",
    "        pool = Pool(processes = n_procs)\n",
    "        outputs = pool.map(_run_command, cmd_info)\n",
    "        pool.terminate()\n",
    "    return outputs\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# pool = Pool(processes=5)\n",
    "# _ = pool.map(preprocess, range(1,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to preprocess to only select O2 within 3.5 angstroms of protein\n",
    "import mdtraj as md\n",
    "import numpy as np\n",
    "import scipy\n",
    "from subprocess import Popen, PIPE\n",
    "import subprocess as sp\n",
    "from biopandas.pdb import PandasPdb\n",
    "import sys\n",
    "\n",
    "# which frames have O2IF within 2.5 angstroms\n",
    "\n",
    "\n",
    "def cmap_per_residue(traj,sel_list,ref_list,cutoff=3.5,metric='euclidean'):\n",
    "    if len(ref_list)==0 or len(sel_list)==0:\n",
    "        print(\"One of the selections is empty\")\n",
    "        return []\n",
    "    coord1 = traj.xyz[:,sel_list,:]*10.0 # gas\n",
    "    coord2 = traj.xyz[:,ref_list,:]*10.0 # protein\n",
    "    residue_sel = np.array([traj.topology.atom(ind).residue.index for ind in sel_list]) # gas\n",
    "    residue_ref = np.array([traj.topology.atom(ind).residue.index for ind in ref_list])\n",
    "    residue_sel_un = np.unique(residue_sel) # gas\n",
    "    residue_ref_un = np.unique(residue_ref)\n",
    "    dic_sel={t:c  for c,t in enumerate(residue_sel_un)} # gas\n",
    "    dic_ref={t:c  for c,t in enumerate(residue_ref_un)}\n",
    "    histo = np.zeros((len(coord1),len(residue_sel_un),len(residue_ref_un)))\n",
    "    for c,j in enumerate(coord1):\n",
    "        axis_sel,axis_ref = np.where(scipy.spatial.distance.cdist(coord1[c],coord2[c],metric=metric) < cutoff)\n",
    "        real_a= residue_sel[axis_sel]\n",
    "        real_b= residue_ref[axis_ref]\n",
    "        elements,counts = np.unique(list(zip(real_a,real_b)),axis=0,return_counts=True)\n",
    "        for c1,el in enumerate(elements):\n",
    "            ax1=dic_sel[el[0]];\n",
    "            ax2=dic_ref[el[1]];\n",
    "            histo[c,ax1,ax2] = counts[c1];\n",
    "    return residue_sel_un,residue_ref_un,histo\n",
    "\n",
    "# gas = sys.argv[1]\n",
    "gas = 'O2IF'\n",
    "gases_ligands = ['O2I','N2I','O2Q','N2']#, 'AKG'\n",
    "simnum=1\n",
    "trjnum=1\n",
    "# top = '/data/pompei/bw973/Oxygenases/PHD2/PHD2_50_O2IF/Bundle/Sim%d/equil5.gro' % simnum\n",
    "nfrcond=1\n",
    "lengths = []\n",
    "top = 'equil5.gro'\n",
    "# trjconv -f $i -s run1.tpr -o an$number.xtc -pbc mol -ur compact -center <<< \"14 0\"\n",
    "# for trjnum in range(1, 5):\n",
    "def preprocess(trjnum, gas='O2IF',check=False):\n",
    "    print(trjnum)\n",
    "    cmds = [\"echo '14 0' | gmx trjconv -f run%d -s run1.tpr -o an%d.xtc -pbc mol -ur compact -center\" % (trjnum,trjnum)]\n",
    "    if check:\n",
    "        if not os.path.exists(\"an%d.xtc\" % trjnum):\n",
    "            _ = run_commands(cmds, supress=True)\n",
    "        else:\n",
    "            print(\"an%d.xtc exists\" % trjnum)\n",
    "    else:\n",
    "        print(\"making an%d.xtc\" % trjnum)\n",
    "        _ = run_commands(cmds, supress=True)\n",
    "    # args = [\"gmx\", \"trjconv\", \"-f\", 'run%d.xtc' % (trjnum), \"-pbc\", \"mol\", \"-ur\",\\\n",
    "    #      \"compact\", \"-center\", '-o', 'an%d.xtc' % (trjnum), \"-s\", \"run1.tpr\" ,  \"<<<\", '14 0']\n",
    "    # p2 = Popen(args, stdout=PIPE, stderr=PIPE)\n",
    "    # stdout, stderr = p2.communicate()\n",
    "    t = '/data/pompei/bw973/Oxygenases/PHD2/PHD2_50_O2IF/Bundle/Sim%d/an%d.xtc' % (simnum,trjnum)\n",
    "    print(\"loading centered an%d.xtc\" % trjnum)\n",
    "    traj = md.load(t, top=top)\n",
    "    time=np.arange(len(traj))*.01\n",
    "    protein = traj.topology.select('protein')\n",
    "    gas2 = traj.topology.select('resname %s' % gas)\n",
    "    AKG = traj.topology.select('resname AKG')\n",
    "\n",
    "    test = cmap_per_residue(traj,gas2,protein,cutoff=2.5)\n",
    "    test_akg = cmap_per_residue(traj,gas2,AKG,cutoff=3.87)\n",
    "    segments_dict = {}\n",
    "    for i in range(0,24):\n",
    "        res=test[2][0:,i]\n",
    "        t=np.zeros(len(traj))\n",
    "        res_akg=test_akg[2][0:,i]\n",
    "        \n",
    "        t[np.concatenate((np.where(res)[0], np.where(res_akg)[0]))] = 1\n",
    "\n",
    "        segment=[]\n",
    "        subsegment,subtime,subframe=[],[],[]\n",
    "        for c,value in enumerate(t):\n",
    "            if value==0 and len(subsegment)==0:\n",
    "                subsegment,subtime=[],[]\n",
    "            elif value==0 and len(subsegment)!=0:\n",
    "                if len(subsegment) > nfrcond: \n",
    "                    segment.append(np.array([subframe,subtime,subsegment]))\n",
    "                subsegment,subtime,subframe=[],[],[]\n",
    "            else: \n",
    "                subsegment.append(value)\n",
    "                subtime.append(time[c])\n",
    "                subframe.append(c)\n",
    "        if len(subsegment) > nfrcond: segment.append(np.array([subframe,subtime,subsegment]))\n",
    "        segments_dict[i] = segment\n",
    "\n",
    "    l=[]\n",
    "    for k in segments_dict.keys():\n",
    "        if len(segments_dict[k]) > 0:\n",
    "            sl = np.concatenate([r[0] for r in segments_dict[k]])\n",
    "            l.append(np.unique(sl))\n",
    "    if len(l) > 0:\n",
    "        frames = np.unique(np.concatenate(l).astype(int))\n",
    "        fn = frames+1\n",
    "        print(len(fn))\n",
    "        lengths.append(len(fn))\n",
    "    else:\n",
    "        print(0)\n",
    "        lengths.append(0)\n",
    "    np.save('lengths2.npy', lengths)\n",
    "    trajf=traj[frames]\n",
    "    print(np.array(frames))\n",
    "\n",
    "    for i,f in enumerate(frames):\n",
    "        trajfn = trajf[i]\n",
    "        l2 = np.array([], dtype=np.int32)\n",
    "        test2 = cmap_per_residue(trajfn,gas2,protein,cutoff=2.5)\n",
    "        test2_akg = cmap_per_residue(trajfn,gas2,AKG,cutoff=5.5)\n",
    "        whr = np.where([j.sum()>0 for j in test2[2][0]])[0]*2\n",
    "        whr_akg = np.where([j.sum()>0 for j in test2_akg[2][0]])[0]*2\n",
    "        l2 = np.append(l2, whr)\n",
    "        l2 = np.append(l2, whr_akg)\n",
    "        # print(i,f,l2)\n",
    "        # break\n",
    "        whr2 = np.concatenate([(n,(n+1)) for n in np.unique(l2)])\n",
    "        # print(whr2)\n",
    "        atms = np.sort(np.concatenate((gas2[whr2],AKG,protein)))\n",
    "        sel = trajfn.atom_slice(atms)\n",
    "        # atms = np.sort(np.concatenate((gas2[whr2],AKG,protein)))\n",
    "        # sel = trajfn.atom_slice(atms)\n",
    "        pdb=\"Sim_%d_traj_%d_frame_%d\" % (simnum, trjnum, f+1)+'.pdb'\n",
    "        sel.save(masif_opts[\"ligand\"][\"assembly_dir\"]+'/'+pdb)\n",
    "        \n",
    "        ppdb_df = PandasPdb().read_pdb(masif_opts[\"ligand\"][\"assembly_dir\"]+'/'+pdb)\n",
    "        o2if = ppdb_df.df['ATOM']\n",
    "        o2if.loc[o2if.residue_name=='O2I','atom_name'] = np.concatenate([['OI1','OI2']] * (len(o2if.loc[o2if.residue_name=='O2I','atom_name'])//2))\n",
    "        ppdb_df._df['ATOM'] = o2if[(o2if['residue_name']!='ACE') & (o2if['residue_name']!='HOH')]\n",
    "        atom = o2if[~o2if.residue_name.isin(gases_ligands)]\n",
    "        hetatm = o2if[o2if.residue_name.isin(gases_ligands)]\n",
    "        ppdb_df._df['ATOM'] = atom\n",
    "        ppdb_df._df['HETATM'] = hetatm\n",
    "        ppdb_df._df['HETATM'].loc[:,'record_name']='HETATM'\n",
    "        ppdb_df.to_pdb(masif_opts[\"ligand\"][\"assembly_dir\"]+'/'+pdb)\n",
    "\n",
    "        df_atom=ppdb_df._df['ATOM']\n",
    "        df=ppdb_df._df['HETATM']\n",
    "        o2i_uniq = np.unique(df[df.residue_name=='O2I'].residue_number)\n",
    "        o2i_uniq = np.sort(o2i_uniq)\n",
    "\n",
    "        lig_types = []\n",
    "        akg_rn = np.unique(df_atom[df_atom.residue_name=='AKG'].residue_number)\n",
    "        lig_types.append(['AKG','res_%d' % akg_rn])\n",
    "        [lig_types.append([gas, 'res_%d' % i]) for i in o2i_uniq]\n",
    "        np.save(masif_opts[\"ligand\"][\"ligand_coords_dir\"]+pdb.split('.')[0]+'_ligtypes.npy', lig_types)\n",
    "\n",
    "        lig_coords = {}\n",
    "        lig_coords[akg_rn[0]] = df[df.residue_name=='AKG'][['x_coord', 'y_coord','z_coord']].values\n",
    "        for val in o2i_uniq:\n",
    "            # print(val)\n",
    "            lig_coords[val] = df[df.residue_number==val][['x_coord', 'y_coord','z_coord']].values\n",
    "\n",
    "        np.save(masif_opts[\"ligand\"][\"ligand_coords_dir\"]+pdb.split('.')[0]+'_ligcoords.npy', lig_coords)\n",
    "    cmds = [\"rm an%d.xtc\" % (trjnum)]\n",
    "    run_commands(cmds, supress=True)\n",
    "# preprocess(trjnum)\n",
    "\n",
    "import sys\n",
    "from functools import partial\n",
    "# python get_close_pdbs.py 2 3 O2IF 2\n",
    "# processes = int(sys.argv[1])\n",
    "# rng = int(sys.argv[2])\n",
    "# pool = Pool(processes=processes)\n",
    "# gas = sys.argv[3]\n",
    "# _ = pool.map(partial(preprocess, gas=gas), range(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = 'equil5.gro'\n",
    "t = '/data/pompei/bw973/Oxygenases/PHD2/PHD2_50_O2IF/Bundle/Sim%d/an%d.xtc' % (simnum,trjnum)\n",
    "print(\"loading centered an%d.xtc\" % trjnum)\n",
    "traj = md.load(t, top=top)\n",
    "time=np.arange(len(traj))*.01\n",
    "protein = traj.topology.select('protein')\n",
    "gas2 = traj.topology.select('resname %s' % gas)\n",
    "AKG = traj.topology.select('resname AKG')\n",
    "\n",
    "test = cmap_per_residue(traj,gas2,protein,cutoff=2.5)\n",
    "test_akg = cmap_per_residue(traj,gas2,AKG,cutoff=3.87)\n",
    "segments_dict = {}\n",
    "for i in range(0,24):\n",
    "    res=test[2][0:,i]\n",
    "    t=np.zeros(len(traj))\n",
    "    res_akg=test_akg[2][0:,i]\n",
    "    \n",
    "    t[np.concatenate((np.where(res)[0], np.where(res_akg)[0]))] = 1\n",
    "\n",
    "    segment=[]\n",
    "    subsegment,subtime,subframe=[],[],[]\n",
    "    for c,value in enumerate(t):\n",
    "        if value==0 and len(subsegment)==0:\n",
    "            subsegment,subtime=[],[]\n",
    "        elif value==0 and len(subsegment)!=0:\n",
    "            if len(subsegment) > nfrcond: \n",
    "                segment.append(np.array([subframe,subtime,subsegment]))\n",
    "            subsegment,subtime,subframe=[],[],[]\n",
    "        else: \n",
    "            subsegment.append(value)\n",
    "            subtime.append(time[c])\n",
    "            subframe.append(c)\n",
    "    if len(subsegment) > nfrcond: segment.append(np.array([subframe,subtime,subsegment]))\n",
    "    segments_dict[i] = segment\n",
    "\n",
    "l=[]\n",
    "for k in segments_dict.keys():\n",
    "    if len(segments_dict[k]) > 0:\n",
    "        sl = np.concatenate([r[0] for r in segments_dict[k]])\n",
    "        l.append(np.unique(sl))\n",
    "if len(l) > 0:\n",
    "    frames = np.unique(np.concatenate(l).astype(int))\n",
    "    fn = frames+1\n",
    "    print(len(fn))\n",
    "    lengths.append(len(fn))\n",
    "else:\n",
    "    print(0)\n",
    "    lengths.append(0)\n",
    "np.save('lengths2.npy', lengths)\n",
    "trajf=traj[frames]\n",
    "print(np.array(frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i,f=list(enumerate(frames))[14]\n",
    "trajfn = trajf[i]\n",
    "l2 = np.array([], dtype=np.int32)\n",
    "test2 = cmap_per_residue(trajfn,gas2,protein,cutoff=2.5)\n",
    "test2_akg = cmap_per_residue(trajfn,gas2,AKG,cutoff=5.5)\n",
    "whr = np.where([j.sum()>0 for j in test2[2][0]])[0]*2\n",
    "whr_akg = np.where([j.sum()>0 for j in test2_akg[2][0]])[0]*2\n",
    "l2 = np.append(l2, whr)\n",
    "l2 = np.append(l2, whr_akg)\n",
    "# print(i,f,l2)\n",
    "# break\n",
    "whr2 = np.concatenate([(n,(n+1)) for n in np.unique(l2)])\n",
    "# print(whr2)\n",
    "atms = np.sort(np.concatenate((gas2[whr2],AKG,protein)))\n",
    "sel = trajfn.atom_slice(atms)\n",
    "# atms = np.sort(np.concatenate((gas2[whr2],AKG,protein)))\n",
    "# sel = trajfn.atom_slice(atms)\n",
    "pdb=\"Sim_%d_traj_%d_frame_%d\" % (simnum, trjnum, f+1)+'.pdb'\n",
    "sel.save(masif_opts[\"ligand\"][\"assembly_dir\"]+'/'+pdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppdb_df = PandasPdb().read_pdb(masif_opts[\"ligand\"][\"assembly_dir\"]+'/'+'Sim_1_traj_1_frame_190.pdb')\n",
    "o2if = ppdb_df.df['ATOM']\n",
    "# o2if.loc[o2if.residue_name=='O2I','atom_name'] = np.concatenate([['OI1','OI2']] * (len(o2if.loc[o2if.residue_name=='O2I','atom_name'])//2))\n",
    "# ppdb_df._df['ATOM'] = o2if[(o2if['residue_name']!='ACE') & (o2if['residue_name']!='HOH')]\n",
    "atom = o2if[~o2if.residue_name.isin(gases_ligands)]\n",
    "hetatm = o2if[o2if.residue_name.isin(gases_ligands)]\n",
    "ppdb_df._df['ATOM'] = atom\n",
    "ppdb_df._df['HETATM'] = hetatm\n",
    "ppdb_df._df['HETATM'].loc[:,'record_name']='HETATM'\n",
    "ppdb_df.to_pdb(masif_opts[\"ligand\"][\"assembly_dir\"]+'/'+pdb)\n",
    "\n",
    "df_atom=ppdb_df._df['ATOM']\n",
    "df=ppdb_df._df['HETATM']\n",
    "o2i_uniq = np.unique(df[df.residue_name=='O2I'].residue_number)\n",
    "o2i_uniq = np.sort(o2i_uniq)\n",
    "\n",
    "lig_types = []\n",
    "akg_rn = np.unique(df_atom[df_atom.residue_name=='AKG'].residue_number)\n",
    "lig_types.append(['AKG','res_%d' % akg_rn])\n",
    "[lig_types.append([gas, 'res_%d' % i]) for i in o2i_uniq]\n",
    "# np.save(masif_opts[\"ligand\"][\"ligand_coords_dir\"]+pdb.split('.')[0]+'_ligtypes.npy', lig_types)\n",
    "\n",
    "lig_coords = {}\n",
    "lig_coords[akg_rn[0]] = df_atom[df_atom.residue_name=='AKG'][['x_coord', 'y_coord','z_coord']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdbs = glob.glob(masif_opts[\"ligand\"][\"assembly_dir\"]+'/*.pdb')\n",
    "# pdbs = [i.split(\"/\")[2] for i in pdbs]\n",
    "# pdbs\n",
    "# np.save(masif_opts[\"ligand\"][\"ligand_coords_dir\"]+pdb.split('.')[0]+'_ligcoords.npy', lig_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb='data_preparation/00b-pdbs_assembly/Sim_1_traj_7_frame_1669.pdb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pdb in pdbs[10:]:\n",
    "# for pdb in ['data_preparation/00b-pdbs_assembly/Sim_1_traj_1_frame_190.pdb']:\n",
    "    # print(pdb)\n",
    "    pref=pdb.split(\"/\")[2].split('.')[0]\n",
    "    print(pref)\n",
    "    ppdb_df = PandasPdb().read_pdb(pdb)\n",
    "    df_atom=ppdb_df._df['ATOM']\n",
    "    lig_coords = np.load(masif_opts[\"ligand\"][\"ligand_coords_dir\"]+pref+'_ligcoords.npy',\n",
    "                         allow_pickle=True).item()\n",
    "    akg_rn = np.unique(df_atom[df_atom.residue_name=='AKG'].residue_number)\n",
    "    lig_coords[akg_rn[0]] = df_atom[df_atom.residue_name=='AKG'][['x_coord', 'y_coord','z_coord']].values\n",
    "    # print(lig_coords[akg_rn[0]])\n",
    "    np.save(masif_opts[\"ligand\"][\"ligand_coords_dir\"]+pref+'_ligcoords.npy', lig_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lig_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymesh\n",
    "import numpy as np\n",
    "\n",
    "# test_mesh = pymesh.load_mesh('/data/pompei/bw973/Oxygenases/masif/data/masif_ligand/data_preparation/01-benchmark_surfaces/1IQP_ACBEDF.ply')\n",
    "test_mesh = pymesh.load_mesh('data_preparation/01-benchmark_surfaces/Sim_2_traj_50_frame_68_protein.ply')\n",
    "test_mesh.get_attribute(\"vertex_iface\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_mesh=pymesh.load_mesh('data_preparation/01-benchmark_surfaces/%s_protein.ply' % pdb)\n",
    "regular_mesh.get_attribute(\"vertex_iface\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb='Sim_1_traj_10_frame_1556'\n",
    "regular_mesh=pymesh.load_mesh('data_preparation/01-benchmark_surfaces/%s_protein.ply' % pdb)\n",
    "regular_mesh.get_attribute(\"vertex_iface\").sum()\n",
    "\n",
    "full_regular_mesh=pymesh.load_mesh('/tmp/%s_protein_complex.ply' % pdb)\n",
    "full_regular_mesh.get_attribute(\"vertex_iface\").sum()\n",
    "\n",
    "regular_names = np.load('data_preparation/01-benchmark_surfaces/%s_names.npy' % pdb, allow_pickle=True).item()\n",
    "\n",
    "full_regular_names = np.load('data_preparation/01-benchmark_surfaces/%s_fullnames.npy' % pdb, allow_pickle=True).item()\n",
    "\n",
    "regular_names=regular_names['names1']\n",
    "full_regular_names=full_regular_names['names1']\n",
    "print(len(full_regular_mesh.vertices), len(regular_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_names['names1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from triangulation.computeMSMS import computeMSMS, computeMSMS2\n",
    "probe='0.70'\n",
    "vertices1, faces1, normals1, names1, areas1 = computeMSMS2(\"/tmp/Sim_1_traj_10_frame_1556_protein.pdb\",\n",
    "        protonate=True, probe=probe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where([re.search('AKG',i) for i in regular_names])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_regular_names\n",
    "\n",
    "vertices2 = vertices1\n",
    "faces2 = faces1\n",
    "from triangulation.computeCharges import computeCharges, assignChargesToNewMesh, assignAtomNamesToNewMesh\n",
    "from triangulation.fixmesh import fix_mesh\n",
    "# Step 4: Fix the mesh.\n",
    "mesh = pymesh.form_mesh(vertices2, faces2)\n",
    "regular_mesh = fix_mesh(mesh, 0.8)\n",
    "\n",
    "regular_names = assignAtomNamesToNewMesh(regular_mesh.vertices, \n",
    "                                          vertices1, names1) \n",
    "regular_names=np.concatenate(regular_names)    \n",
    "np.where([re.search('AKG',i) for i in regular_names])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (regular_mesh.get_attribute(\"vertex_iface\")==2).sum()\n",
    "(regular_mesh.get_attribute(\"vertex_iface\")==1).sum()\n",
    "# (regular_mesh.get_attribute(\"vertex_iface\")).sum()\n",
    "mesh=full_regular_mesh\n",
    "import re\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_iface = np.zeros(len(full_regular_names))\n",
    "full_iface[np.where([re.search('O2I',i) for i in full_regular_names])[0]] = 1\n",
    "\n",
    " # vertices from full mesh with gas/everything\n",
    "O = np.where([re.search('O2I',i) for i in full_regular_names])[0]\n",
    "tO=mesh.vertices[O]\n",
    "# vertices from regular mesh of protein/cofactor\n",
    "A=np.where([re.search('AKG',i) for i in regular_names])[0]\n",
    "tA=regular_mesh.vertices[A]\n",
    "\n",
    "t2=scipy.spatial.distance_matrix(regular_mesh.vertices,tO)\n",
    "t3=scipy.spatial.distance_matrix(regular_mesh.vertices,tA)\n",
    "iface1=np.where([(i<3).sum() for i in t2])[0] # protein res near gas\n",
    "iface2=np.where([(i<5).sum() for i in t3])[0] # protein res near cofactor\n",
    "iface3=np.where([(i<2).sum() for i in t2])[0] # protein res VERY near gas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "t2=scipy.spatial.distance_matrix(regular_mesh.vertices,tO)\n",
    "# np.where([(i<1).sum() for i in t2])\n",
    "iface1=regular_names[np.where([(i<3).sum() for i in t2])]\n",
    "iface1=np.where([(i<3).sum() for i in t2])[0]\n",
    "len(iface1)\n",
    "# regular_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_mesh.get_attribute(\"vertex_cofactor_dist\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_near_gas_to_AKG[iface1].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t3=scipy.spatial.distance_matrix(regular_mesh.vertices,tA)\n",
    "# np.where([(i<1).sum() for i in t2])\n",
    "\n",
    "iface2=np.where([(i<5).sum() for i in t3])[0]\n",
    "len(iface2)\n",
    "regular_names[np.where([(i<2).sum() for i in t3])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_names = np.load('data_preparation/01-benchmark_surfaces/%s_names.npy' % pdb, allow_pickle=True).item()['names1']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from masif_modules.read_data_from_surface2 import read_data_from_surface, compute_shape_complementarity\n",
    "import pymesh\n",
    "from sklearn.neighbors import KDTree\n",
    "import numpy as np\n",
    "from triangulation.fixmesh import fix_mesh\n",
    "from default_config.masif_opts import masif_opts\n",
    "\n",
    "\n",
    "params = masif_opts['ligand']\n",
    "\n",
    "\n",
    "\n",
    "sufs = ['protein','gas']\n",
    "\n",
    "# pdb=sys.argv[1]\n",
    "pdb='Sim_1_traj_10_frame_1042'\n",
    "fields = pdb.split('.')\n",
    "pid=fields[0]\n",
    "# masif_app = sys.argv[2]\n",
    "\n",
    "rho = {}\n",
    "neigh_indices = {}\n",
    "mask = {}\n",
    "input_feat = {}\n",
    "theta = {}\n",
    "iface_labels = {}\n",
    "verts = {}\n",
    "\n",
    "\n",
    "# for suf in sufs:\n",
    "    # ply_file = masif_opts[\"ply_chain_dir\"] + \"/{}_protein.ply\".format(pid)\n",
    "suf=\"protein\"\n",
    "p_ply_file = masif_opts[\"ply_chain_dir\"] + \"{}_{}.ply\".format(pid,suf)\n",
    "print(p_ply_file)\n",
    "input_feat[suf], rho[suf], theta[suf], mask[suf], neigh_indices[suf], iface_labels[suf], verts[suf] = read_data_from_surface(p_ply_file, params, pid=pid+\"_\"+suf)\n",
    "print(len(verts[suf]))\n",
    "\n",
    "suf=\"gas\"\n",
    "g_ply_file = masif_opts[\"ply_chain_dir\"] + \"{}_{}.ply\".format(pid,suf)\n",
    "print(g_ply_file)\n",
    "input_feat[suf], rho[suf], theta[suf], mask[suf], neigh_indices[suf], iface_labels[suf], verts[suf] = read_data_from_surface(g_ply_file, params, False, pid+\"_\"+suf)\n",
    "print(len(verts[suf]))\n",
    "\n",
    "# print(pid)\n",
    "#         print(ply_file[pid])\n",
    "#         input_feat[pid], rho[pid], theta[pid], mask[pid], neigh_indices[pid], iface_labels[pid], verts[pid] = read_data_from_surface(ply_file[pid], params, my_precomp_dir+pid)\n",
    "#         print(len(verts[pid]))\n",
    "#         pad=params[\"max_shape_size\"]\n",
    "#         neigh_indices[pid]= np.array([i + [-1]*(pad-len(i)) for i in neigh_indices[pid]])\n",
    "\n",
    "\n",
    "\n",
    "# pid=pids[0]\n",
    "# input_feat[pid], rho[pid], theta[pid], mask[pid], neigh_indices[pid], iface_labels[pid], verts[pid] = read_data_from_surface(ply_file[pid], params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_mesh=pymesh.load_mesh(p_ply_file)\n",
    "regular_mesh.get_attribute(\"vertex_iface\")==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(i) for i in neigh_indices[suf]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = masif_opts['ppi_search']\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = masif_opts['ligand']\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[regular_mesh.get_attribute(\"vertex_iface\")==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_names\n",
    "mylabels = p1_sc_labels[0]\n",
    "labels = np.median(mylabels, axis=1)\n",
    "np.where((labels < 1) & (labels > 0.5))[0]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_precomp_dir+pid+'prot_sc_labels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['sc_w']=0.015\n",
    "params['sc_interaction_cutoff']=3.0\n",
    "params['sc_radius']=params['max_distance']\n",
    "my_precomp_dir = params['masif_precomputation_dir']\n",
    "\n",
    "p1_sc_labels, p2_sc_labels = compute_shape_complementarity(p_ply_file, g_ply_file, neigh_indices['protein'],neigh_indices['gas'], rho['protein'], rho['gas'], mask['protein'], mask['gas'], params)\n",
    "np.save(my_precomp_dir+pid+'_prot_sc_labels', p1_sc_labels)\n",
    "np.save(my_precomp_dir+pid+'_gas_sc_labels', p2_sc_labels)\n",
    "mylabels = p1_sc_labels[0]\n",
    "labels = np.median(mylabels, axis=1)\n",
    "print(labels.max())\n",
    "labels[regular_mesh.get_attribute(\"vertex_iface\")==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(regular_mesh.get_attribute(\"vertex_iface\")==1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_names[pos_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mylabels = p1_sc_labels[0]\n",
    "labels = np.median(mylabels, axis=1)\n",
    "print(pid, labels.max())\n",
    "labels[regular_mesh.get_attribute(\"vertex_iface\")==1]\n",
    "\n",
    "\n",
    "pos_labels = np.where(((labels < -0.2) & (labels > -0.6) | (labels > 0.1)))[0]\n",
    "print(pid,\"len(pos_labels)\", len(pos_labels))\n",
    "regular_names[pos_labels]\n",
    "\n",
    "print(pid, \"len_intersect_iface\",len(np.intersect1d(\n",
    "    np.where(regular_mesh.get_attribute(\"vertex_iface\")==1),\n",
    "    pos_labels\n",
    ")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_labels = np.where(((labels < 0) & (labels > -0.65) | (labels > 0.65)))[0]\n",
    "print(len(pos_labels))\n",
    "regular_names[pos_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(regular_mesh.get_attribute(\"vertex_iface\")==1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(labels[regular_mesh.get_attribute(\"vertex_iface\")==1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2_sc_labels[0].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suf=\"protein\"\n",
    "ply_file = masif_opts[\"ply_chain_dir\"] + \"{}_{}.ply\".format(pid,suf)\n",
    "print(ply_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.load(\"/data/pompei/bw973/Oxygenases/masif/source/test/test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA\n",
    "import os\n",
    "os.path.basename(\"data/pompei/bw973/Oxygenases/masif/source/test/test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymol.cgo import *\n",
    "\n",
    "obj = []\n",
    "obj.extend([BEGIN, 0.5])\n",
    "obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iface3=np.where([(i<2).sum() for i in t2])[0]\n",
    "regular_names[np.intersect1d(iface3, iface2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_names[np.intersect1d(iface3, iface2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names=np.concatenate(np.load('data_preparation/01-benchmark_surfaces/Sim_2_traj_42_frame_1367_names.npy',\n",
    "    allow_pickle=True).item()['names1'])\n",
    "len(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Header variables and parameters.\n",
    "import sys\n",
    "import pymesh\n",
    "import os\n",
    "import numpy as np\n",
    "from IPython.core.debugger import set_trace\n",
    "import importlib\n",
    "\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "from default_config.masif_opts import masif_opts\n",
    "\n",
    "\"\"\"\n",
    "masif_ppi_search_cache_training_data.py: Function to cache all the training data for MaSIF-search. \n",
    "                This function extract all the positive pairs and a random number of negative surfaces.\n",
    "                In the future, the number of negative surfaces should be increased.\n",
    "Pablo Gainza - LPDI STI EPFL 2019\n",
    "Released under an Apache License 2.0\n",
    "\"\"\"\n",
    "\n",
    "params = masif_opts['ppi_search']\n",
    "\n",
    "custom_params = {}\n",
    "custom_params['cache_dir'] = 'nn_models/sc05/cache/'\n",
    "custom_params['model_dir'] = 'nn_models/sc05/all_feat/model_data/'\n",
    "custom_params['desc_dir'] = 'output/sc05/all_feat/model_data/'\n",
    "custom_params['gif_eval_out'] = 'nn_models/sc05/gif_eval/'\n",
    "custom_params['min_sc_filt'] = 0.5\n",
    "custom_params['max_sc_filt'] = 1.0\n",
    "custom_params['pos_surf_accept_probability'] = 1.0\n",
    "\n",
    "for key in custom_params: \n",
    "    print('Setting {} to {} '.format(key, custom_params[key]))\n",
    "    params[key] = custom_params[key]\n",
    "\n",
    "if 'pids' not in params: \n",
    "    params['pids'] = ['p1', 'p2']\n",
    "\n",
    "\n",
    "# Read the positive first \n",
    "parent_in_dir = params['masif_precomputation_dir']\n",
    "\n",
    "binder_rho_wrt_center = []\n",
    "binder_theta_wrt_center = []\n",
    "binder_input_feat = []\n",
    "binder_mask = []\n",
    "\n",
    "pos_rho_wrt_center = []\n",
    "pos_theta_wrt_center = []\n",
    "pos_input_feat = []\n",
    "pos_mask = []\n",
    "\n",
    "neg_rho_wrt_center = []\n",
    "neg_theta_wrt_center = []\n",
    "neg_input_feat = []\n",
    "neg_mask = []\n",
    "\n",
    "np.random.seed(0)\n",
    "training_idx = []\n",
    "val_idx = []\n",
    "test_idx = []\n",
    "pos_names = []\n",
    "neg_names = []\n",
    "\n",
    "training_list = [x.rstrip() for x in open(params['training_list']).readlines()]\n",
    "testing_list = [x.rstrip() for x in open(params['testing_list']).readlines()]\n",
    "\n",
    "idx_count = 0\n",
    "for count, ppi_pair_id in enumerate(os.listdir(parent_in_dir)[0:10]):\n",
    "    if ppi_pair_id not in testing_list and ppi_pair_id not in training_list:\n",
    "        continue\n",
    "    in_dir = parent_in_dir + ppi_pair_id+'/'\n",
    "    print(ppi_pair_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppi_pair_id=\"Sim_1_traj_42_frame_1246\"\n",
    "in_dir = parent_in_dir + ppi_pair_id+'/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.load(in_dir+ppi_pair_id+'_prot_sc_labels.npy')\n",
    "# Take the median of the percentile 25 shape complementarity.\n",
    "mylabels = labels[0]\n",
    "labels = np.median(mylabels, axis=1)\n",
    "pos_labels = np.where(((labels < -0.2) & (labels > -0.6) | (labels > 0.1)))[0]\n",
    "K = int(params['pos_surf_accept_probability']*len(pos_labels))\n",
    "l = np.arange(len(pos_labels))\n",
    "np.random.shuffle(l)\n",
    "l = l[:K] \n",
    "l = pos_labels[l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pos_labels)\n",
    "ply_fn1=masif_opts['ply_file_template'].format(ppi_pair_id,\"protein\")\n",
    "ply_fn2=masif_opts['ply_file_template'].format(ppi_pair_id,\"gas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=pymesh.load_mesh(ply_fn1)\n",
    "m.get_attribute(\"vertex_iface\").sum()\n",
    "\n",
    "\n",
    "v0 = pymesh.load_mesh(ply_fn1).vertices\n",
    "v1 = v0[l]\n",
    "v2 = pymesh.load_mesh(ply_fn2).vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "\n",
    "t2=scipy.spatial.distance_matrix(v0,v2)\n",
    "names=np.load(masif_opts['ply_chain_dir']+ppi_pair_id+'_names.npy', allow_pickle=True).item()\n",
    "len(names['names1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(names['names1'][m.get_attribute(\"vertex_iface\")==1]))\n",
    "names['names1'][m.get_attribute(\"vertex_iface\")==2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=[all(i>20) for i in t2]\n",
    "print(sum(test))\n",
    "names['names1'][test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# far from oxygen and far from cofactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contact_points = np.where(d < params['pos_interface_cutoff'])[0]\n",
    "l[contact_points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho_wrt_center = np.load(in_dir+ppi_pair_id+'_rho_wrt_center.npy')\n",
    "rho_wrt_center.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdt = cKDTree(v1)\n",
    "dneg, rneg = kdt.query(v2)\n",
    "dneg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dneg), len(v1), len(v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdt = cKDTree(v0)\n",
    "dneg, rneg = kdt.query(v2)\n",
    "dneg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    kdt = cKDTree(v1)\n",
    "    dneg, rneg = kdt.query(v2)\n",
    "except:\n",
    "    set_trace()\n",
    "k_neg2 = np.where(dneg > params['pos_interface_cutoff']*2)[0]\n",
    "k_neg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(d < 2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdt.query?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdt = cKDTree(v2)\n",
    "d,r= kdt.query(v1)\n",
    "# Contact points: those within a cutoff distance.\n",
    "contact_points = np.where(d < params['pos_interface_cutoff'])[0]\n",
    "try:\n",
    "    k1 = l[contact_points]\n",
    "except:\n",
    "k2 = r[contact_points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "v1 = pymesh.load_mesh(ply_fn1)\n",
    "v2 = pymesh.load_mesh(ply_fn2)\n",
    "# v1 = pymesh.load_mesh(ply_fn1).vertices[l]\n",
    "# v2 = pymesh.load_mesh(ply_fn2).vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the corresponding ply files. \n",
    "fields = ppi_pair_id.split('_')\n",
    "ply_fn1 = masif_opts['ply_file_template'].format(fields[0], fields[1])\n",
    "ply_fn2 = masif_opts['ply_file_template'].format(fields[0], fields[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_names = np.load('data_preparation/01-benchmark_surfaces/Sim_2_traj_42_frame_1410_names.npy',\n",
    "    allow_pickle=True).item()\n",
    "\n",
    "regular_mesh=pymesh.load_mesh('data_preparation/01-benchmark_surfaces/Sim_2_traj_42_frame_1410_protein.ply')\n",
    "regular_mesh.get_attribute(\"vertex_iface\").sum()\n",
    "\n",
    "full_regular_mesh=pymesh.load_mesh('data_preparation/01-benchmark_surfaces/Sim_2_traj_42_frame_1410_protein_complex.ply')\n",
    "full_regular_mesh.get_attribute(\"vertex_iface\").sum()\n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_names['names1'][np.where([(i<4).sum() for i in t2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where([re.search('A_210',i) for i in regular_names])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KDTree\n",
    "\n",
    "kdt = KDTree(v3)\n",
    "d, r = kdt.query(regular_mesh.vertices)\n",
    "d = np.square(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(d==max(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.max()\n",
    "np.where(d>=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_names['names1'][np.where(d>=0.5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.PDB import *\n",
    "\n",
    "function\n",
    "# Exclude disordered atoms.\n",
    "class NotDisordered(Select):\n",
    "    def accept_atom(self, atom):\n",
    "        return not atom.is_disordered() or atom.get_altloc() == \"A\"  or atom.get_altloc() == \"1\" \n",
    "\n",
    "tmp_dir= masif_opts['tmp_dir']\n",
    "out_filename1=out_filename1 = tmp_dir+\"/\"+pdb\n",
    "infilename=pdb_filename\n",
    "outfilename=out_filename1\n",
    "\n",
    "parser = PDBParser(QUIET=True)\n",
    "struct = parser.get_structure(infilename, infilename)\n",
    "\n",
    "model = Selection.unfold_entities(struct, \"M\")[0]\n",
    "structBuild = StructureBuilder.StructureBuilder()\n",
    "structBuild.init_structure(\"output\")\n",
    "structBuild.init_seg(\" \")\n",
    "structBuild.init_model(0)\n",
    "outputStruct = structBuild.get_structure()\n",
    "\n",
    "for chain in model:\n",
    "    structBuild.init_chain(chain.get_id())\n",
    "    for residue in chain:\n",
    "        het = residue.get_id()\n",
    "        if het[0] == \" \":\n",
    "            outputStruct[0][chain.get_id()].add(residue)\n",
    "pdbio = PDBIO()\n",
    "pdbio.set_structure(outputStruct)\n",
    "pdbio.save(outfilename, select=NotDisordered())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Selection.unfold_entities(struct, \"M\")[0]\n",
    "structBuild = StructureBuilder.StructureBuilder()\n",
    "structBuild.init_structure(\"output\")\n",
    "structBuild.init_seg(\" \")\n",
    "structBuild.init_model(0)\n",
    "outputStruct = structBuild.get_structure()\n",
    "\n",
    "for chain in model:\n",
    "    structBuild.init_chain(chain.get_id())\n",
    "    for residue in chain:\n",
    "        het = residue.get_id()\n",
    "        if het[0] == \" \":\n",
    "            outputStruct[0][chain.get_id()].add(residue)\n",
    "pdbio = PDBIO()\n",
    "pdbio.set_structure(outputStruct)\n",
    "pdbio.save(outfilename, select=NotDisordered())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gases_ligands = ['O2I', 'AKG']\n",
    "ppdb_df._df['HETATM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel.save(masif_opts[\"ligand\"][\"assembly_dir\"]+pdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel.save(masif_opts[\"ligand\"][\"assembly_dir\"]+pdb)\n",
    "ppdb_df = PandasPdb().read_pdb(masif_opts[\"ligand\"][\"assembly_dir\"]+pdb)\n",
    "o2if = ppdb_df.df['ATOM']\n",
    "o2if[o2if.residue_name.isin(gases_ligands)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppdb_df.df['HETATM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppdb_df._df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sel.save(masif_opts[\"ligand\"][\"assembly_dir\"]+pdb)\n",
    "         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppdb_df = PandasPdb().read_pdb(masif_opts[\"ligand\"][\"assembly_dir\"]+pdb)\n",
    "o2if = ppdb_df.df['ATOM']\n",
    "o2if.loc[o2if.residue_name=='O2I','atom_name'] = np.concatenate([['OI1','OI2']] * (len(o2if.loc[o2if.residue_name=='O2I','atom_name'])//2))\n",
    "ppdb_df._df['ATOM'] = o2if[(o2if['residue_name']!='ACE') & (o2if['residue_name']!='HOH')]\n",
    "ppdb_df.to_pdb(masif_opts[\"ligand\"][\"assembly_dir\"]+'/'+pdb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which frames have O2IF within 2.5 angstroms of protein or within 9 of AKG\n",
    "protein = traj.topology.select('protein')\n",
    "gas2 = traj.topology.select('resname %s' % gas)\n",
    "AKG = traj.topology.select('resname AKG')\n",
    "\n",
    "\n",
    "def cmap_per_residue(traj,sel_list,ref_list,cutoff=3.5,metric='euclidean'):\n",
    "    if len(ref_list)==0 or len(sel_list)==0:\n",
    "        print(\"One of the selections is empty\")\n",
    "        return []\n",
    "    coord1 = traj.xyz[:,sel_list,:]*10.0 # gas\n",
    "    coord2 = traj.xyz[:,ref_list,:]*10.0 # protein\n",
    "    residue_sel = np.array([traj.topology.atom(ind).residue.index for ind in sel_list]) # gas\n",
    "    residue_ref = np.array([traj.topology.atom(ind).residue.index for ind in ref_list])\n",
    "    residue_sel_un = np.unique(residue_sel) # gas\n",
    "    residue_ref_un = np.unique(residue_ref)\n",
    "    dic_sel={t:c  for c,t in enumerate(residue_sel_un)} # gas\n",
    "    dic_ref={t:c  for c,t in enumerate(residue_ref_un)}\n",
    "    histo = np.zeros((len(coord1),len(residue_sel_un),len(residue_ref_un)))\n",
    "    for c,j in enumerate(coord1):\n",
    "        axis_sel,axis_ref = np.where(scipy.spatial.distance.cdist(coord1[c],coord2[c],metric=metric) < cutoff)\n",
    "        real_a= residue_sel[axis_sel]\n",
    "        real_b= residue_ref[axis_ref]\n",
    "        elements,counts = np.unique(list(zip(real_a,real_b)),axis=0,return_counts=True)\n",
    "        for c1,el in enumerate(elements):\n",
    "            ax1=dic_sel[el[0]];\n",
    "            ax2=dic_ref[el[1]];\n",
    "            histo[c,ax1,ax2] = counts[c1];\n",
    "    return residue_sel_un,residue_ref_un,histo\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time=np.arange(len(traj))*.01\n",
    "len(time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = cmap_per_residue(traj,gas2,protein,cutoff=2.5)\n",
    "nfrcond=2\n",
    "\n",
    "test = cmap_per_residue(traj,gas2,protein,cutoff=2.5)\n",
    "segments_dict = {}\n",
    "for i in range(0,24):\n",
    "    res=test[2][0:,i]\n",
    "    t=np.zeros(len(traj))\n",
    "    t[np.where(res)[0]] = 1\n",
    "\n",
    "    segment=[]\n",
    "    subsegment,subtime,subframe=[],[],[]\n",
    "    for c,value in enumerate(t):\n",
    "        if value==0 and len(subsegment)==0:\n",
    "            subsegment,subtime=[],[]\n",
    "        elif value==0 and len(subsegment)!=0:\n",
    "            if len(subsegment) > nfrcond: \n",
    "                segment.append(np.array([subframe,subtime,subsegment]))\n",
    "            subsegment,subtime,subframe=[],[],[]\n",
    "        else: \n",
    "            subsegment.append(value)\n",
    "            subtime.append(time[c])\n",
    "            subframe.append(c)\n",
    "    if len(subsegment) > nfrcond: segment.append(np.array([subframe,subtime,subsegment]))\n",
    "    segments_dict[i] = segment\n",
    "\n",
    "l=[]\n",
    "for k in segments_dict.keys():\n",
    "    if len(segments_dict[k]) > 0:\n",
    "        sl = np.concatenate([r[0] for r in segments_dict[k]])\n",
    "        l.append(np.unique(sl))\n",
    "frames = np.unique(np.concatenate(l).astype(int))\n",
    "fn = frames+1\n",
    "print(fn)\n",
    "trajf=traj[frames]\n",
    "\n",
    "for i,f in enumerate(frames):\n",
    "    trajfn = trajf[i]\n",
    "    test2 = cmap_per_residue(trajfn,gas2,protein,cutoff=2.5)\n",
    "    whr = np.where([j.sum()>0 for j in test2[2][0]])[0]*2\n",
    "    whr2 = np.concatenate([(n,(n+1)) for n in whr])\n",
    "    print(whr2)\n",
    "    atms = np.sort(np.concatenate((gas2[whr2],AKG,protein)))\n",
    "    sel = trajfn.atom_slice(atms)\n",
    "    atms = np.sort(np.concatenate((gas2[whr2],AKG,protein)))\n",
    "    sel = trajfn.atom_slice(atms)\n",
    "    pdb=\"Sim_%d_traj_%d_frame_%d\" % (simnum, trjnum, f+1)+'.pdb'\n",
    "    sel.save(masif_opts[\"ligand\"][\"assembly_dir\"]+pdb)\n",
    "    \n",
    "    ppdb_df = PandasPdb().read_pdb(masif_opts[\"ligand\"][\"assembly_dir\"]+pdb)\n",
    "    o2if = ppdb_df.df['ATOM']\n",
    "    o2if.loc[o2if.residue_name=='O2I','atom_name'] = np.concatenate([['OI1','OI2']] * (len(o2if.loc[o2if.residue_name=='O2I','atom_name'])//2))\n",
    "    ppdb_df._df['ATOM'] = o2if[(o2if['residue_name']!='ACE') & (o2if['residue_name']!='HOH')]\n",
    "    ppdb_df.to_pdb(masif_opts[\"ligand\"][\"assembly_dir\"]+'/'+pdb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,f in enumerate(frames):\n",
    "    trajfn = trajf[i]\n",
    "    test2 = cmap_per_residue(trajfn,gas2,protein,cutoff=2.5)\n",
    "    whr = np.where([j.sum()>0 for j in test2[2][0]])[0]*2\n",
    "    whr2 = np.concatenate([(n,(n+1)) for n in whr])\n",
    "    print(whr2)\n",
    "    atms = np.sort(np.concatenate((gas2[whr2],AKG,protein)))\n",
    "    sel = trajfn.atom_slice(atms)\n",
    "    atms = np.sort(np.concatenate((gas2[whr2],AKG,protein)))\n",
    "    sel = trajfn.atom_slice(atms)\n",
    "    pdb=\"Sim_%d_traj_%d_frame_%d\" % (simnum, trjnum, f+1)+'.pdb'\n",
    "    sel.save(masif_opts[\"ligand\"][\"assembly_dir\"]+pdb)\n",
    "    \n",
    "    ppdb_df = PandasPdb().read_pdb(masif_opts[\"ligand\"][\"assembly_dir\"]+pdb)\n",
    "    o2if = ppdb_df.df['ATOM']\n",
    "    o2if.loc[o2if.residue_name=='O2I','atom_name'] = np.concatenate([['OI1','OI2']] * (len(o2if.loc[o2if.residue_name=='O2I','atom_name'])//2))\n",
    "    ppdb_df._df['ATOM'] = o2if[(o2if['residue_name']!='ACE') & (o2if['residue_name']!='HOH')]\n",
    "    ppdb_df.to_pdb(masif_opts[\"ligand\"][\"assembly_dir\"]+'/'+pdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=ppdb_df._df['ATOM']\n",
    "o2i_uniq = np.unique(df[df.residue_name=='O2I'].residue_number)\n",
    "o2i_uniq = np.sort(o2i_uniq)\n",
    "\n",
    "lig_types = []\n",
    "akg_rn = np.unique(df[df.residue_name=='AKG'].residue_number)\n",
    "lig_types.append(['AKG','res_%d' % akg_rn])\n",
    "[lig_types.append([gas, 'res_%d' % i]) for i in o2i_uniq]\n",
    "np.save(masif_opts[\"ligand\"][\"ligand_coords_dir\"]+pdb.split('.')[0]+'_ligtypes.npy', lig_types)\n",
    "\n",
    "lig_coords = {}\n",
    "lig_coords[akg_rn[0]] = df[df.residue_name=='AKG'][['x_coord', 'y_coord','z_coord']].values\n",
    "for val in o2i_uniq:\n",
    "    print(val)\n",
    "    lig_coords[val] = df[df.residue_number==val][['x_coord', 'y_coord','z_coord']].values\n",
    "\n",
    "np.save(masif_opts[\"ligand\"][\"ligand_coords_dir\"]+pdb.split('.')[0]+'_ligcoords.npy', lig_coords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "rm $prefix.pdb\n",
    "files=$(ls Sim_2*_traj_2*frame**.pdb | sort -V)\n",
    "prefix=Sim_2_traj_2\n",
    "for f in $files; do\n",
    "    echo $(wc -l $f)\n",
    "    frame=$(echo $f | awk -F_ '{print $NF}' | cut -d. -f1)\n",
    "    echo \"MODEL $frame\" >> $prefix.pdb\n",
    "    cat $f | grep -v MODEL | grep -v 'END$' >> $prefix.pdb\n",
    "done\n",
    "echo END >> $prefix.pdb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "holoprot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
